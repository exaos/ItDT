9 Data Processing

In previous chapters, we have encountered some useful tools for specific computing tasks. For example, in Chapter 7 we learned about SQL for extracting data from a relational database.
Given a data set that has been stored in a relational database, we could now write a piece of SQL code to extract data from that data set. But what if we want to extract data from the database at a specific time of day? What if we want to repeat that task every day for a year? SQL has no concept of when to execute a task.
In Chapter 5, we learned about the benefits of the XML language for storing data, but we also learned that it is a very verbose format. We could conceive of designing the structure of an XML document, but would we really be prepared to write large amounts of XML by hand? What we want to be able to do is to get the computer to write the file of XML code for us.
In order to perform these sorts of tasks, we need a programming language. With a programming language, we will be able to tell the computer to perform a task at a certain time, or to repeat a task a certain number of times. We will be able to create files of information and we will be able to perform calculations with data values.
The purpose of this chapter is to explore and enumerate some of the tasks that a programming language will allow us to perform. As with previous topics, it is important to be aware of what tasks are actually possible as well as look at the technical details of how to carry out each task.
As we might expect, a programming language will let us do a lot more than the specific languages like HTML, XML, and SQL can do, but this will come at a cost because we will need to learn a few more complex concepts. However, the potential benefit is limitless. This is the chapter where we truly realize the promise of taking control of our computing environment.
The other important purpose of this chapter is to introduce a specific programming language, so that we can perform tasks in practice.
There are many programming languages to choose from, but in this chapter we will use the R language because it is relatively simple to learn and because it is particularly well suited to working with data.

Computer hardware
So, what can we do with a programming language?
To answer that question, it will again be useful to compare and contrast a programming language with the more specific languages that we have already met and what we can do with those.
SQL lets us talk to database systems; we can ask database software to extract specific data from database tables. With HTML, we can talk to web browsers; we can instruct the browser software to draw certain content on a web page. Working with XML is a bit more promiscuous because we are essentially speaking to any software system that might be interested in our data. However, we are limited to only being able to say “here are the data”.
A programming language is more general and more powerful than any of these, and the main reason for this is because a programming language allows us to talk not just to other software systems, but also to the computer hardware.
In order to understand the significance of this, we need to have a very basic understanding of the fundamental components of computer hardware and what we can do with them. Figure 9.1 shows a simple diagram of the basic components in a standard computing environment, and each component is briefly described below.

Figure 9.1: A diagram illustrating the basic components of a standard computing environment.

The Central Processing Unit (CPU) is the part of the hardware that can perform calculations or process our data. It is only capable of a small set of operations—basic arithmetic, plus the ability to compare values, and the ability to shift values between locations in computer memory—but it performs these tasks exceptionally quickly. Complex tasks are performed by combining many, many simple operations in the CPU.
The CPU also has access to a clock for determining times.
Being able to talk to the CPU means that we can perform arbitrarily complex calculations. This starts with simple things like determining minima, maxima, and averages for numeric values, but includes sophisticated data analysis techniques, and there really is no upper bound.
Computer memory, the hardware where data values are stored, comes in many different forms. Random Access Memory (RAM) is the term usually used to describe the memory that sits closest to the CPU. This memory is temporary—data values in RAM only last while the computer is running (they disappear when the computer is turned off)—and it is fast—values can be transferred to the CPU for processing and the result can be transferred back to RAM very rapidly. RAM is also usually relatively small.
Loosely speaking, RAM corresponds to the popular notion of short-term memory.
All processing of data typically involves, at a minimum, both RAM and the CPU. Data values are stored temporarily in RAM, shifted to the CPU to perform arithmetic or comparisons or something more complex, and the result is then stored back in RAM. A fundamental feature of a programming language is the ability to store values in RAM and specify the calculations that should be carried out by the CPU.
Being able to store data in RAM means that we can accomplish a complex task by performing a series of simpler steps. After each step, we record the intermediate result in memory so that we can use that result in subsequent steps.

The keyboard is one example of input hardware.
Most computers also have a mouse or touchpad. These are also examples of input hardware, but for the purposes of this book we are mostly interested in being able to enter code and data via a keyboard.
We will not be writing code to control the keyboard; this hardware component is more relevant to us as the primary way in which we will communicate our instructions and data to the computer.
Most computers typically have a very large repository of computer memory, such as a hard drive. This will be much larger and much slower than RAM but has the significant advantage that data values will persist when the power goes off. Mass storage is
where we save all of our files and documents.
A related set of hardware components includes external storage devices, such as CD and DVD drives and thumb drives (memory sticks), which allow the data to be physically transported away from the machine.
Where RAM corresponds to short-term memory, mass storage corresponds to long-term memory.
It is essential to be able to access mass storage because that is where the original data values will normally reside. With access to mass storage, we can also permanently store the results of our data processing as well as our computer code.
The computer screen is one example of output hardware. The screen is important as the place where text and images are
displayed to show the results of our calculations.
Being able to control what is displayed on screen is important for viewing the results of our calculations and possibly for sharing those results with others.

Most modern computers are connected to a network of some kind, which consists of other computers, printers, etc,
and in many cases the general internet.
As with mass storage, the importance of having access to the network is that this may be where the original data values reside.
A programming language will allow us to work with these hardware components to perform all manner of useful tasks, such as reading files or documents from a hard disk into RAM, calculating new values, and displaying those new values on the computer screen.
How this chapter is organized
This chapter begins with a task that we might naturally think to perform by hand, but which can be carried out much more efficiently and accurately if instead we write code in a programming language to perform the task. The aims of this section are to show how useful a little programming knowledge can be and to demonstrate how an overall task can be broken down into smaller tasks that we can perform by writing code.
Sections 9.2 and 9.3 provide an initial introduction to the R programming language for performing these sorts of tasks. These sections will allow us to write and run some very basic R code.
Section 9.4 introduces the important idea of data structures—how data values are stored in RAM. In this section, we will learn how to enter data values by hand and how those values can be organized. All data processing tasks require data values to be loaded into RAM before we can perform calculations on the data values. Different processing tasks will require the data values to be organized and stored in different ways, so it is important to understand what options exist for storing data in RAM and how to work with each of the available options.
Some additional details about data structures are provided in Section 9.6, but before that, Section 9.5 provides a look at one of the most basic data processing tasks, which is extracting a subset from a large set of values. Being able to break a large data set into smaller pieces is one of the fundamental small steps that we can perform with a programming language. Solutions to more complex tasks are based on combinations of these small steps.
Section 9.7 addresses how to get data from external files into RAM so that we can process them. Most data sets are stored permanently in some form of mass storage, so it is crucial to know how to load data from various storage formats into RAM using R.
Section 9.8 describes a number of data processing tasks. Much of the chapter up to this point is laying the foundation. This section starts to provide information for performing powerful calculations with data values. Again, the individual techniques that we learn in these sections provide the foundation for completing more complex tasks. Section 9.8.12 provides a larger case study that demonstrates how the smaller steps can be combined to carry out a more substantial data processing exercise.
Section 9.9 looks at the special case of processing text data. We will look at tools for searching within text, extracting subsets from text, and splitting and recombining text values. Section 9.9.2 describes regular expressions, which are an important tool for searching for patterns within text.
Section 9.10 describes how to format the results of data processing, either for display on screen or for use in research reports.
Section 9.11 very briefly discusses some more advanced ideas about writing code in a programming language, and Section 10 contains a few comments about and pointers to alternative software and programming languages for data processing. These are more advanced topics and provide a small glimpse of areas to explore further.
9.1 Case study: The Population Clock
The U.S. Census Bureau maintains a web site called the World Population Clock (see Figure 9.2).
This web site provides an up-to-the-minute snapshot of the world’s population, based on estimates by the U.S. Census Bureau. It is updated every few seconds.
In this case study, we will use this clock to generate a rough estimate of the current rate of growth of the world’s population.
We will perform this task by taking two snapshots of the World Population Clock, ten minutes apart, and then we will divide the change in population by the change in time.
The purpose of this section is to look at the steps involved in detail, noting how we might perform this task “by hand”, and then looking at how we might use the computer to do the work instead. This will allow us to see what sorts of tasks we can expect a computer to be able to perform and will begin to introduce some of the programming concepts involved.

Figure 9.2: The World Population Clock web page shows an up-to-the-minute snapshot of the world’s population (based on estimates by the U.S. Census Bureau).


1. Copy the current value of the population clock.
The first step is to capture a snapshot of the world population from the U.S. Census Bureau web site.
This is very easy to do by hand; we simply navigate a web browser to the population clock web page and type out, cut-and-paste, or even just write down the current population value.
What about getting the computer to do the work?
Navigating to a web page and downloading the information is not actually very difficult. This is an example of interacting with the network component of the computing environment. Downloading a web page is something that we can expect any modern programming language to be able to do, given the appropriate URL (which is visible in the “navigation bar” of the web browser in Figure 9.2).
The following R code will perform this data import task. > clockHTML <-
                      readLines("http://www.census.gov/ipc/www/popclockworld.html")
We will not focus on understanding all of the details of the examples of R code in this section—that is the purpose of the remainder of this chapter. The code is just provided here as concrete evidence that the task can be done and as a simple visual indication of the level of effort and complexity involved.
Conceptually, the above code says “read the HTML code from the network location given by the URL and store it in RAM under the name clockHTML.” The images below illustrate this idea, showing how the information that we input at the keyboard (the URL) leads to the location of a file containing HTML code on the network, which is read into RAM and given the name clockHTML. The image on the left shows the main hardware components involved in this process in general and the image on the right shows the actual data values and files involved in this particular example. We will use diagrams like this throughout the chapter to illustrate which hardware components we are dealing with when we perform different tasks.

It is important to realize that the result of the R code above is not a nice picture of the web page like we see in a browser. Instead, we have the raw HTML code that describes the web page (see Figure 9.3).
This is actually a good thing because it would be incredibly difficult for the computer to extract the population information from a picture.
The HTML code is better than a picture because the HTML code has a clear structure. If information has a pattern or structure, it is much easier to write computer code to navigate within the information. We will exploit the structure in the HTML code to get the computer to extract the relevant population value for us.
However, before we do anything with this HTML code, it is worth taking note of what sort of information we have. From Chapter 2,

            <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
                "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
            <html xmlns="http://www.w3.org/1999/xhtml"
                  xml:lang="en" lang="en">
            <head>
                <title>World POPClock Projection</title>
                <link rel="stylesheet"
                      href="popclockworld%20Files/style.css"
                      type="text/css">
                <meta name="author" content="Population Division">
                <meta http-equiv="Content-Type"
                      content="text/html; charset=iso-8859-1">
                <meta name="keywords" content="world, population">
                <meta name="description"
                      content="current world population estimate">
                <style type="text/css">
                    #worldnumber {
                text-align: center;
                font-weight: bold;
                font-size: 400%;
                color: #ff0000;
} </style>
            </head>
            <body>
                <div id="cb_header">
                <a href="http://www.census.gov/">
                <img src="popclockworld%20Files/cb_head.gif"
                     alt="U.S. Census Bureau"
                     border="0" height="25" width="639">
                </a>
                </div>
                <h1>World POPClock Projection</h1>
                <p></p>
                According to the <a href="http://www.census.gov/ipc/www/">
                International Programs Center</a>, U.S. Census Bureau,
                the total population of the World, projected to 09/12/07
                at 07:05 GMT (EST+5) is<br><br>
                <div id="worldnumber">6,617,746,521</div>
                <p></p>
                <hr>
...
Figure 9.3: Part of the HTML code for the World Population Clock web page (see Figure 9.2). The line numbers (in grey) are just for reference.

we know that HTML code is just plain text, so what we have downloaded is a plain text file. This means that, in order to extract the world population value from the HTML code, we will need to know something about how to perform text processing. We are going to need to search within the text to find the piece we want, and we are going to need to extract just that piece from the larger body of text.
The current population value on the web page is contained within the HTML code in a div tag that has an id attribute, with the unique value "worldnumber" (line 41 in Figure 9.3). This makes it very easy to find the line that contains the population estimate because we just need to search for the pattern id="worldnumber". This text search task can be performed using the following code:
> popLineNum <- grep('id="worldnumber"', clockHTML)
This code says “find the line of HTML code that contains the text id="worldnumber" and store the answer in RAM under the name popLineNum.” The HTML code is fetched from RAM, we supply the pattern to search for by typing it at the keyboard, the computer searches the HTML code for our pattern and finds the matching line, and the result of our search is stored back in RAM.

We can see the value that has been stored in RAM by typing the appropriate name.
                > popLineNum
                [1] 41
Notice that the result this time is not text; it is a number representing the appropriate line within the HTML code.
Also notice that each time we store a value in RAM, we provide a label for the value so that we can access the value again later. We stored the complete set of HTML code with the label clockHTML, and we have now also stored the result of our search with the label popLineNum.
What we want is the actual line of HTML code rather than just the number telling us which line, so we need to use popLineNum to extract a subset of the text in clockHTML. This action is performed by the following code.
                > popLine <- clockHTML[popLineNum]
Again, this task involves using information that we already have in RAM to calculate a new data value, and we store the new value back in RAM with the label popLine.

As before, we can just type the name to see the value that has been stored in RAM. The new value in this case is a line of text.
> popLine
                [1] "    <div id=\"worldnumber\">6,617,746,521</div>"
In many of the code examples throughout this chapter, we will follow this pattern: in one step, calculate a value and store it in RAM, with a label; then, in a second step, type the name of the label to display the value that has been stored.
Now that we have the important line of HTML code, we want to extract just the number, 6,617,746,521, from that line. This task consists of getting rid of the HTML tags. This is a text search-and-replace task and can be performed using the following code:
> popText <- gsub('^.*<div id="worldnumber">|</div>.*$', "", popLine)
                > popText
                [1] "6,617,746,521"
This code says “delete the start and end div tags (and any spaces in front of the start tag)”. We have used a regular expression, '^.*<div id="worldnumber">|</div>.*$', to specify the part of the text that we want to get rid of, and we have specified "", which means an empty piece of text, as the text to replace it with.

Section 9.9 describes text processing tasks and regular expressions in more detail.
At this point, we are close to having what we want, but we are not quite there yet because the value that we have for the world’s population is still a piece of text, not a number. This is a very important point. We always need to be aware of exactly what sort of information we are dealing with. As described in Chapter 5, computers represent different sorts of values in different ways, and certain operations are only possible with certain types of data. For example, we ultimately want to be able to perform arithmetic on the population value that we are getting from this web site. That means that we must have a number; it does not make sense to perform arithmetic with text values.
Thus, the final thing we need to do is turn the text of the population estimate into a number so that we can later carry out mathematical operations. This process is called type coercion and appropriate code is shown below.
                > pop <- as.numeric(gsub(",", "", popText))
                > pop
                [1] 6617746521
Notice that we have to process the text still further to remove the commas that are so useful for human viewers but a complete distraction for computers.

And now we have what we were after: the current U.S. Census Bureau estimate of the world’s population from the World Population Clock web site.
This first step provides a classic demonstration of the difference between performing a task by hand and writing code to get a computer to do the work. The manual method is simple, requires no new skills, and takes very little time. On the other hand, the computer code approach requires learning new information (it will take substantial chunks of this chapter to explain the code we have used so far), so it is more difficult and takes longer (the first time). However, the computer code approach will pay off in the long run, as we are about to see.

2. Wait ten minutes.
The second step involves letting time pass so that we can obtain a second snapshot of the world population after a fixed time interval.
Doing nothing is about as simple as it gets for a do-it-yourself task. However, it highlights two of the major advantages of automating tasks by computer. First, computers will perform boring tasks without complaining or falling asleep, and, second, their accuracy will not degrade as a function of the boredom of the task.
The following code will make the computer wait for 10 minutes (600 seconds):
                > Sys.sleep(600)
3. Copy the new value of the population clock.
The third step is to take another snapshot of the world population from the U.S. Census Bureau web site.
This is the same as the first task. If we do it by hand, it is just as easy as it was before, though tasks like this quickly become tiresome if we have to repeat them many times.
What about doing it by computer code?
Here we see a third major benefit of writing computer code: once code has been written to perform a task, repetitions of the task become essentially free. All of the pain of writing the code in the first place starts to pay off very rapidly once a task has to be repeated. Almost exactly the same code as before will produce the new population clock estimate.
                  > clockHTML2 <-
                      readLines("http://www.census.gov/ipc/www/popclockworld.html")
> popLineNum2 <- grep('id="worldnumber"', clockHTML2)
> popLine2 <- clockHTML2[popLineNum2]
> popText2 <- gsub('^.*<div id="worldnumber">|</div>.*$',
                                    "", popLine2)
                > pop2 <- as.numeric(gsub(",", "", popText2))
> pop2
                [1] 6617747987

One detail that we have ignored to this point is the fact that the results of our calculations are being printed out. The information that we have stored in RAM is being displayed on the screen. As this example suggests, there may be differences between the value that is stored in memory and what is actually displayed for human eyes; in this case, the computer displays an “index”, [1], in front of the number. This is another important point to be aware of as we proceed through this chapter.
4. Calculate the growth rate.
The fourth step in our task is to divide the change in the population estimate by the time interval.
This is a very simple calculation that is, again, easy to do by hand.

But arithmetic like this is just as easy to write code for. All we need to do is divide the change in population by the elapsed time (10 minutes):
                > rateEstimate <- (pop2 - pop)/10
                > rateEstimate
[1] 146.6
The final result is this: at the time of writing, we estimate that the world population was growing at the rate of about 147 people every minute.

As a final step, it would be prudent to save this result in a more permanent state, by writing this information to more permanent computer memory. The values that are in RAM will disappear when we quit from R. The following code creates a new text file and stores our rate estimate in that file.
                > writeLines(as.character(rateEstimate),
                              "popRate.txt")
Notice that, in this step, we start with a number and convert it to a text value so that it can be stored as part of a text file.
To reiterate, although that may seem like quite a lot of work to go through to perform a relatively simple task, the effort is worth it. By writing code so that the computer performs the task, we can improve our accuracy and efficiency, and we can repeat the task whenever we like for no additional cost. For example, we might want to improve our estimate of the population growth rate by taking several more snapshots from the population clock web site. This would take hours by hand, but we have most of the code already, and with a little more knowledge we could set the computer happily working away on this task for us.
This chapter is concerned with writing code like this, using the R language, to conduct and automate general data handling tasks: importing and exporting data, manipulating the shape of the data, and processing data into new forms.
In the following sections, we will begin to look specifically at how to perform these tasks in R.
9.2 The R environment
The name R is used to describe both the R language and the R software
environment that is used to run code written in the language.
In this section, we will give a brief introduction to the R software. We will
discuss the R language from Section 9.3 onwards.
The R software can be run on Windows, MacOS X, and Linux. An appropriate version may be downloaded from the Comprehensive R Archive Network (CRAN).1 The user interface will vary between these settings, but the crucial common denominator that we need to know about is the command line.
Figure 9.4 shows what the command line looks like on Windows and on Linux.
9.2.1 The command line
The R command line interface consists of a prompt, usually the > character. We type code written in the R language and, when we press Enter, the code is run and the result is printed out. A very simple interaction with the command line looks like this:
1 http://cran.r-project.org/

>1+3+5+7
[1] 16
Throughout this chapter, examples of R code will displayed like this, with the R code preceded by a prompt, >, and the results of the code (if any) displayed below the code. The format of the displayed result will vary because there can be many different kinds of results from running R code.
In this case, a simple arithmetic expression has been typed and the numeric result has been printed out.
Notice that the result is not being stored in memory. We will look at how to retain results in memory in Section 9.3.

One way to write R code is simply to enter it interactively at the command line as shown above. This interactivity is beneficial for experimenting with R or for exploring a data set in a casual manner. For example, if we want to determine the result of division by zero in R, we can quickly find out by just trying it.
> 1/0
[1] Inf
However, interactively typing code at the R command line is a very bad approach from the perspective of recording and documenting code because the code is lost when R is shut down.
A superior approach in general is to write R code in a file and get R to read the code from the file.
cut-and-paste
One way to work is to write R code in a text editor and then cut-and-paste bits of the code from the text editor into R. Some editors can be associated with an R session and allow submission of code chunks via a single key stroke (e.g., the Windows GUI provides a script editor with this facility).
source()
Another option is to read an entire file of R code into R using the source() function (see Section 10.3.8). For example, if we have a file called code.R containing R code, then we can run the R code by typing the following at the R command line:
                > source("code.R")
R reads the code from the file and runs it, one line at a time.

Whether there is any output and where it goes (to the screen, to RAM, or to mass storage) depends on the contents of the R code.
We will look at starting to write code using the R language in Section 9.3, but there is one example of R code that we need to know straight away. This is the code that allows us to exit from the R environment: to do this, we type q().
9.2.2 The workspace
When quitting R, the option is given to save the “workspace image”.

The workspace consists of all values that have been created during a session— all of the data values that have been stored in RAM.
The workspace is saved as a file called .Rdata and when R starts up, it checks for such a file in the current working directory and loads it automatically. This provides a simple way of retaining the results of calculations from one R session to the next.
However, saving the entire R workspace is not the recommended approach. It is better to save the original data set and R code and recreate results by running the code again.
If we have specific results that we want to save permanently to mass storage, for example, the final results of a large and time-consuming analysis, we can use the techniques described later in Sections 9.7 and 9.10.
The most important point for now is that we should save any code that we write; if we always know how we got a result, we can always recreate the result later, if necessary.
9.2.3 Packages
The features of R are organized into separate bundles called packages. The standard R installation includes about 25 of these packages, but many more can be downloaded from CRAN and installed to expand the things that R can do. For example, there is a package called XML that adds features for working with XML documents in R. We can install that package by typing the following code.
           > install.packages("XML")
Once a package has been installed, it must then be loaded within an R session to make the extra features available. For example, to make use of the XML package, we need to type the following code.
           > library("XML")
Of the 25 packages that are installed by default, nine packages are loaded by default when we start a new R session; these provide the basic functionality of R. All other packages must be loaded before the relevant features can be used.
Recap
The R environment is the software used to run R code.
R code is submitted to the R environment either by typing it directly at the command line, by cutting-and-pasting from a text file containing R code, or by specifying an entire file of R code to run.
R functionality is contained in packages. New functionality can be added by installing and then loading extra packages.

9.3 The R language
R is a popular programming language for working with and analyzing data.
As with the other computer languages that we have dealt with in this book, we have two main topics to cover for R: we need to learn the correct syntax for R code, so that we can write code that will run; and we need to learn the semantics of R code, so that we can make the computer do what we want it to do.
R is a programming language, which means that we can achieve a much greater variety of results with R compared to the other languages that we have seen. The cost of this flexibility is that the syntax for R is more complex and, because there are many more things that we can do, the R vocabulary that we have to learn is much larger.
In this section, we will look at the basics of R syntax—how to write correct R code. Each subsequent section will tackle the meaning of R code by focusing on a specific category of tasks and how they are performed using R.
9.3.1 Expressions
R code consists of one or more expressions.
An expression is an instruction to perform a particular task. For example, the following expression instructs R to add the first four odd numbers together.
>1+3+5+7
[1] 16
If there are several expressions, they are run, one at a time, in the order they appear.
The next few sections describe the basic types of R expressions.
9.3.2 Constant values
The simplest sort of R expression is just a constant value, typically a numeric value (a number) or a character value (a piece of text). For example, if we need to specify a number of seconds corresponding to 10 minutes, we specify a number.
> 600
[1] 600
If we need to specify the name of a file that we want to read data from, we specify the name as a character value. Character values must be surrounded by either double-quotes or single-quotes.
           > "http://www.census.gov/ipc/www/popclockworld.html"
           [1] "http://www.census.gov/ipc/www/popclockworld.html"
As shown above, the result of a constant expression is just the corresponding value and the result of an expression is usually printed out on the screen.

9.3.3 Arithmetic
An example of a slightly more complex expression is an arithmetic expression for calculating with numbers. R has the standard arithmetic operators:
+ addition.
- subtraction.
* multiplication. / division.
^ exponentiation.
For example, the following code shows the arithmetic calculation that was performed in Section 9.1 to obtain the rate of growth of the world’s population—the change in population divided by the elapsed time. Note the use of parentheses to control the order of evaluation. R obeys the normal BODMAS rules of precedence for arithmetic operators, but parentheses are a useful way of avoiding any ambiguity, especially for a human audience.
           > (6617747987 - 6617746521) / 10
           [1] 146.6

9.3.4 Conditions
A condition is an expression that has a yes/no answer—for example, whether one data value is greater than, less than, or equal to another. The result of a condition is a logical value: either TRUE or FALSE.
R has the standard operators for comparing values, plus operators for combining conditions:
==
> and >= < and <= !=
&& || !
equality.
greater than (or equal to). less than (or equal to). inequality.
logical and. logical or. logical not.
For example, the following code asks whether the second population estimate is larger than the first.
> pop2 > pop
[1] TRUE
The code below asks whether the second population estimate is larger than the first and the first population estimate is greater than 6 billion.
> (pop2 > pop) && (pop > 6000000000)
[1] TRUE
The parentheses in this code are not necessary, but they make the code easier to read.
9.3.5 Function calls
The most common and most useful type of R expression is a function call. Function calls are very important because they are how we use R to perform any non-trivial task.
A function call is essentially a complex instruction, and there are thousands of different R functions that perform different tasks. This section just looks at the basic structure of a function call; we will meet some important specific functions for data manipulation in later sections.
A function call consists of the function name followed by, within parentheses and separated from each other by commas, expressions called arguments that provide necessary information for the function to perform its task.
The following code gives an example of a function call that makes R pause for 10 minutes (600 seconds).
           > Sys.sleep(600)
The various components of this function call are shown below:
function name: Sys.sleep(600) parentheses: Sys.sleep(600) argument: Sys.sleep(600)
The name of the function in this example is Sys.sleep (this function makes the computer wait, or “sleep”, for a number of seconds). There is one argument to the function, the number of of seconds to wait, and in this case the value supplied for this argument is 600.
Because function calls are so common and important, it is worth looking at a few more examples to show some of the variations in their format.
The writeLines() function saves text values into an external file. This function has two arguments: the text to save and the name of the file that the text will be saved into. The following expression shows a call to writeLines() that writes the text "146.6" into a file called popRate.txt.
           > writeLines("146.6", "popRate.txt")
The components of this function call are shown below:
function name: parentheses: comma between arguments: first argument: second argument:
writeLines("146.6", "popRate.txt")
writeLines("146.6", "popRate.txt")
writeLines("146.6", "popRate.txt")
writeLines("146.6", "popRate.txt")
writeLines("146.6", "popRate.txt")
This example demonstrates that commas must be placed between arguments in a function call. The first argument is the text to save, "146.6", and the second argument is the name of the text file, "popRate.txt".
The next example is very similar to the previous function call (the result is identical), but it demonstrates the fact that every argument has a name and these names can be specified as part of the function call.
           > writeLines(text="146.6", con="popRate.txt")
The important new
first arg. name:
first arg. value: second arg. name: second arg. value:
components of this function call are shown below:
  writeLines(text="146.6", con="popRate.txt")
  writeLines(text="146.6", con="popRate.txt")
  writeLines(text="146.6", con="popRate.txt")
  writeLines(text="146.6", con="popRate.txt")
When arguments are named in a function call, they may be given in any order, so the previous function call would also work like this:
> writeLines(con="popRate.txt", text="146.6")
The final example in this section is another variation on the function call to writeLines().
> writeLines("146.6")
146.6
The point of this example is that we can call the writeLines() function with only one argument. This demonstrates that some arguments have a default value, and if no value is supplied for the argument in the function call, then the default value is used. In this case, the default value of the con argument is a special value that means that the text is displayed on the screen rather than being saved to a file.
There are many other examples of function calls in the code examples throughout the remainder of this chapter.

9.3.6 Symbols and assignment
Anything that we type that starts with a letter, and which is not one of the special R keywords, is interpreted by R as a symbol (or name).
A symbol is a label for an object that is currently stored in RAM. When R encounters a symbol, it extracts from memory the value that has been stored with that label.
R automatically loads some predefined values into memory, with associated symbols. One example is the predefined symbol pi, which is a label for the the mathematical constant π.
                 > pi
                 [1] 3.141593
The result of any expression can be assigned to a symbol, which means that the result is stored in RAM, with the symbol as its label.
For example, the following code performs an arithmetic calculation and stores the result in memory under the name rateEstimate.
                 > rateEstimate <- (6617747987 - 6617746521) / 10

The combination of a less-than sign followed by a dash, <-, is called the assignment operator. We say that the symbol rateEstimate is assigned the value of the arithmetic expression.
Notice that R does not display any result after an assignment.
When we refer to the symbol rateEstimate, R will retrieve the corresponding value from RAM. An expression just consisting of a symbol will result in the value that is stored with that name being printed on the screen.
           > rateEstimate
           [1] 146.6

In many of the code examples throughout this chapter, we will follow this pattern: in one step, we will calculate a value and assign it to a symbol, which produces no display; then, in a second step, we will type the name of the symbol on its own to display the calculated value.
We can also use symbols as arguments to a function call, as in the following expression, which converts the numeric rate value into a text value.
           > as.character(rateEstimate)
           [1] "146.6"

The value that has been assigned to rateEstimate, 146.6, is retrieved from RAM and passed to the as.character() function, which produces a text version of the number.
In this case, we do not assign the result of our calculation, so it is automatically displayed on the screen.
For non-trivial tasks, assigning values to symbols is vital because we will need to perform several steps in order to achieve the ultimate goal. Assignments allow us to save intermediate steps along the way.
9.3.7 Keywords
Some symbols are used to represent special values. These predefined symbols cannot be reassigned.
NA
Inf
> 1/0
[1] Inf NaN
This symbol is used to represent an arithmetic result that is undefined (Not A Number).
> 0/0
[1] NaN NULL
This symbol is used to represent an empty result. Some R functions do not produce a result, so they return NULL.
TRUE and FALSE
These symbols represent the logical values “true” and “false”. The result of a condition is a logical value.
                > pop2 > pop
                [1] TRUE
This symbol represents a missing or unknown value.
This symbol is used to represent infinity (as the result of an arithmetic expression).

9.3.8 Flashback: Writing for an audience
Chapter 2 introduced general principles for writing computer code. In this section, we will look at some specific issues related to writing scripts in R.
The same principles, such as commenting code and laying out code so that it is easy for a human audience to read, still apply. In R, a comment is anything on a line after the special hash character, #. For example, the comment in the following line of code is useful as a reminder of why the number 600 has been chosen.
                 Sys.sleep(600) # Wait 10 minutes
Indenting is also very important. We need to consider indenting whenever an expression is too long and has to be broken across several lines of code. The example below shows a standard approach that left-aligns all arguments to a function call.
popText <- gsub('^.*<div id="worldnumber">|</div>.*$', "", popLine)
It is also important to make use of whitespace. Examples in the code above include the use of spaces around the assignment operator (<-), around arithmetic operators, and between arguments (after the comma) in function calls.
9.3.9 Naming variables
When writing R code, because we are constantly assigning intermediate values to symbols, we are forced to come up with lots of different symbol names. It is important that we choose sensible symbol names for several reasons:
1. Good symbol names are a form of documentation in themselves. A name like dateOfBirth tells the reader a lot more about what value has been assigned to the symbol than a name like d, or dob, or even date.
2. Short or convenient symbol names, such as x, or xx, or xxx should be avoided because it too easy to create errors by reusing the same name for two different purposes.

Anyone with children will know how difficult it can be to come up with even one good name, let alone a constant supply. However, unlike children, symbols usually have a specific purpose, so the symbol name naturally arises from a description of that purpose. A good symbol name should fully and accurately represent the information that has been assigned to that symbol.
One problem that arises is that a good symbol name is often a combination of two or more words. One approach to making such symbols readable is to use a mixture of lowercase and uppercase letters when typing the name; treat the symbol name like a sentence and start each new word with a capital letter (e.g., dateOfBirth). This naming mechanism is called “camelCase” (the uppercase letters form humps like the back of a camel).
Recap
A programming language is very flexible and powerful because it allows us to control the computer hardware as well as computer software.
R is a programming language with good facilities for working with data. An instruction in the R language is called an expression.
Important types of expressions are: constant values, arithmetic expressions, function calls, and assignments.
Constant values are just numbers and pieces of text.
Arithmetic expressions are typed mostly as they would be written, except for division and exponentiation operators.
Function calls are instructions to perform a specific task and are of the form:
functionName(argument1, argument1)
An assignment saves a value in computer memory with an associated label, called a symbol, so that it can be retrieved again later. An assignment is of the form:
symbol <- expression
R code should be written in a disciplined fashion just like any other computer
code.

9.4 Data types and data structures
We now have some knowledge of R syntax—what R expressions look like. Before we can start to learn some specific R expressions for particular data processing tasks, we first need to spend some time looking at how information is stored in computer memory.
When we are writing code in a programming language, we work most of the time with RAM, combining and restructuring data values to produce new values in RAM.
In Chapter 5, we looked at a number of different data storage formats.
In that discussion, we were dealing with long-term, persistent storage of information on mass storage computer memory.
Although, in this chapter, we will be working in RAM rather than with mass storage, we have exactly the same issues that we had in Chapter 5 of how to represent data values in computer memory. The computer memory in RAM is a series of 0’s and 1’s, just like the computer memory used to store files in mass storage. In order to work with data values, we need to get those values into RAM in some format.
At the basic level of representing a single number or a single piece of text, the solution is the same as it was in Chapter 5. Everything is represented as a pattern of bits, using various numbers of bytes for different sorts of values. In R, in an English locale, and on a 32-bit operating system, a single character usually takes up one byte, an integer takes four bytes, and a real number 8 bytes. Data values are stored in different ways depending on the data type—whether the values are numbers or text.
Although we do not often encounter the details of the memory representation, except when we need a rough estimate of how much RAM a dataset might require, it is important to keep in mind what sort of data type we are working with because the computer code that we write will produce different results for different data types. For example, we can only calculate an average if we are dealing with values that have been stored as numbers, not if the values have been stored as text.
Another important issue is how collections of values are stored in memory. The tasks that we will consider will typically involve working with an entire data set, or an entire variable from a data set, rather than just a single value, so we need to have a way to represent several related values in memory.
This is similar to the problem of deciding on a storage format for a data set, as we discussed in Chapter 5. However, rather than talking about different file formats, in this chapter we will talk about different data structures for storing a collection of data values in RAM. In this section, we will learn about the most common data structures in R.
Throughout this entire chapter, it will be important to always keep clear in our minds what data type we are working with and what sort of data structure are we working with.
Basic data types
Every individual data value has a data type that tells us what sort of value it is. The most common data types are numbers, which R calls numeric values, and text, which R calls character values.
We will meet some other data types as we progress through this section.
Basic data structures
We will look at five basic data structures that are available in R. Vectors
A collection of values that all have the same data type. The elements of a vector are all numbers, giving a numeric vector, or all character values, giving a character vector.
A vector can be used to represent a single variable in a data set.

Factors
A collection of values that all come from a fixed set of possible values. A factor is similar to a vector, except that the values within a factor are limited to a fixed set of possible values.
A factor can be used to represent a categorical variable in a data set.
Matrices
A two-dimensional collection of values that all have the same type. The values are arranged in rows and columns.
There is also an array data structure that extends this idea to more than two dimensions.
Data frames
A collection of vectors that all have the same length. This is like a matrix, except that each column can contain a different data type.
A data frame can be used to represent an entire data set.
Lists
Starting with the next section, we will use a simple case study to explore the memory representation options available to us. We will also look at some of the functions that are used to create different data structures.
A collection of data structures. The components of a list can be simply vectors—similar to a data frame, but with each column allowed to have a different length. However, a list can also be a much more complicated structure.
This is a very flexible data structure. Lists can be used to store any combination of data values together.

Table 9.1: The results from counting how many different sorts of candy there are in a bag of candy. There are 36 candies in total.
Shape Pattern
round pattern light 2 oval pattern light 0 long pattern light 3 round plain light 1 oval plain light 3 long plain light 2 round pattern dark 9 oval pattern dark 0 long pattern dark 2 round plain dark 1
Shade Count
oval plain long plain
dark 11 dark 2
9.4.1 Case study: Counting candy
Table 9.1 shows the results of counting how many different sorts of candy there are in a bag of candy. The candies are categorized by their shape (round, oval, or long), their shade (light or dark), and whether they are plain or have a pattern.
In this example, we have information in a table that we can see (on a printed page or on a computer screen) and we want to enter this information into RAM by typing the values on the computer keyboard. We will look at how to write R code to store the information as data structures within RAM.
We will start by entering the first column of values from Table 9.1—the different shapes of candy. This will demonstrate the c() function for storing data as vectors.
                 > shapes <- c("round", "oval", "long",
                                "round", "oval", "long",
                                "round", "oval", "long",
                                "round", "oval", "long")
> shapes
                  [1] "round" "oval"  "long"  "round" "oval"  "long"  "round"
                  [8] "oval"  "long"  "round" "oval"  "long"

The information in the first column consists of text, so each individual value is entered as a character value (within quotes), and the overall result is a character vector.
The result has been assigned to the symbol shapes so that we can use this character vector again later.
The second and third columns from Table 9.1 can be stored as character vectors in a similar manner.
           > patterns <- c("pattern", "pattern", "pattern",
                            "plain", "plain", "plain",
                            "pattern", "pattern", "pattern",
                            "plain", "plain", "plain")
> patterns
            [1] "pattern" "pattern" "pattern" "plain"   "plain"
            [6] "plain"   "pattern" "pattern" "pattern" "plain"
           [11] "plain"   "plain"
           > shades <- c("light", "light", "light",
                          "light", "light", "light",
                          "dark", "dark", "dark",
                          "dark", "dark", "dark")
> shades
            [1] "light" "light" "light" "light" "light" "light" "dark"
            [8] "dark"  "dark"  "dark"  "dark"  "dark"
The c() function also works with numeric values. In the following code, we create a numeric vector to store the fourth column of Table 9.1.
           > counts <- c(2, 0, 3, 1, 3, 2, 9, 0, 2, 1, 11, 2)
           > counts
            [1]  2  0  3  1  3  2  9  0  2  1 11  2
We now have the information from Table 9.1 stored as four vectors in RAM.

9.4.2 Vectors
The previous example demonstrated the c() function for concatenating values together to create a vector. In this section, we will look at some other functions that create vectors.
When data values have a regular pattern, the function rep() is extremely useful (rep is short for “repeat”). For example, column 1 of Table 9.1 has a simple pattern: the set of three possible shapes is repeated four times. The following code generates the shapes vector again, but this time using rep().
           > shapes <- rep(c("round", "oval", "long"), 4)
           > shapes
            [1] "round" "oval"  "long"  "round" "oval"  "long"  "round"
            [8] "oval"  "long"  "round" "oval"  "long"
The first argument to rep() is the vector of values to repeat and the second argument says how many times to repeat the vector. The result is the original vector of 3 values repeated 4 times, producing a final vector with 12 values.
It becomes easier to keep track of what sort of data structure we are dealing with once we become familiar with the way that R displays the different types of data structures. With vectors, R displays an index inside square brackets at the start of each line of output, followed by the values, which are formatted so that they each take up the same amount of space (similar to a fixed-width file format). The previous result had room to display up to seven values on each row, so the second row of output starts with the eighth value (hence the [8] at the start of the line). All of the values have double-quotes around them to signal that these are all character values (i.e., this is a character vector).
As another example of the use of rep(), the following code generates a data structure containing the values in column 2 of Table 9.1.
           > patterns <- rep(c("pattern", "plain"), each=3, length=12)
           > patterns
            [1] "pattern" "pattern" "pattern" "plain"   "plain"
            [6] "plain"   "pattern" "pattern" "pattern" "plain"
           [11] "plain"   "plain"

This example demonstrates two other arguments to rep(). The each argument says that each individual element of the original vector should be repeated 3 times. The length argument says that the final result should be a vector of length 12 (without that, the 2 original values would only be repeated 3 times each to produce a vector of 6 values; try it and see!).
To complete the set of variables in the candy data set, the following code generates the shade information from column 3 of Table 9.1.
           > shades <- rep(c("light", "dark"), each=6)
           > shades
            [1] "light" "light" "light" "light" "light" "light" "dark"
            [8] "dark"  "dark"  "dark"  "dark"  "dark"
The rep() function also works for generating numeric vectors; another important function for generating regular patterns of numeric values is the seq() function (seq is short for “sequence”). For example, a numeric vector containing the first 10 positive integers can be created with the following code.
           > seq(1, 10)
            [1]  1  2  3  4  5  6  7  8  9 10
The first argument to seq() specifies the number to start at and the second argument specifies the number to finish at.
There is also a by argument to allow steps in the sequence greater than one, as shown by the following code.
           > seq(1, 10, by=3)
[1] 1 4 710
For integer sequences in steps of 1, a short-hand equivalent is available using the special colon operator, :. For example, we could also have generated the first 10 positive integers with the code below.
           > 1:10
            [1]  1  2  3  4  5  6  7  8  9 10

Going back to the candy data, now that we have Table 9.1 stored as four vectors, we can begin to ask some questions about these data. This will allow us to show that when we perform a calculation with a vector of values, the result is often a new vector of values.
As an example, we will look at how to determine which types of candy did not appear in our bag of candy; in other words, we want to find the values in the counts vector that are equal to zero. The following code performs this calculation using a comparison.
> counts == 0
            [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE
           [10] FALSE FALSE FALSE
This result is interesting for three reasons. The first point is that the result of comparing two numbers for equality is a logical value: either TRUE or FALSE. Logical values are another of the basic data types in R. This result above is a logical vector containing only TRUE or FALSE values.
The second point is that we are starting with a numeric vector, counts, which contains 12 values, and we end up with a new vector that also has 12 values. In general, operations on vectors produce vectors as the result, so a very common way of generating new vectors is to do something with an existing vector.
The third point is that the 12 values in counts are being compared to a single numeric value, 0. The effect is to compare each of the 12 values separately against 0 and return 12 answers. This happens a lot in R when two vectors of different lengths are used. Section 9.6.1 discusses this idea of
“recycling” shorter vectors further.
9.4.3 Factors
A factor is a basic data structure in R that is ideal for storing categorical data.
For example, consider the shapes vector that we created previously. This was just a character vector recording the text "round" for counts of round candies, "oval" for counts of oval candies, and "long" for counts of long candies.
This is not the ideal way to store this information because it does not acknowledge that elements containing the same text, e.g., "round", really are the same value. A character vector can contain any text at all, so there are no data integrity constraints. The information would be represented better using a factor.
The following code creates the candy shape information as a factor:
           > shapesFactor <- factor(shapes,
                                     levels=c("round", "oval", "long"))
           > shapesFactor
            [1] round oval  long  round oval  long  round oval  long
           [10] round oval  long
           Levels: round oval long
The first argument to the factor() function is the set of data values. We have also specified the set of valid values for the factor via the levels argument.
This is a better representation of the data because every value in the factor shapesFactor is now guaranteed to be one of the valid levels of the factor.
Factors are displayed in a similar way to vectors, but with additional information displayed about the levels of the factor.
9.4.4 Data frames
A vector in R contains values that are all of the same type. Vectors correspond to a single variable in a data set.
Most data sets consist of more than just one variable, so to store a complete data set we need a different data structure. In R, several variables can be stored together in an object called a data frame.
We will now build a data frame to contain all four variables in the candy data set (i.e., all of the information in Table 9.1).
The function data.frame() creates a data frame object from a set of vectors. For example, the following code generates a data frame from the variables that we have previously created, shapes, patterns, shades, and counts.

           > candy <- data.frame(shapes, patterns, shades, counts)
           > candy
              shapes patterns shades counts
           1   round  pattern  light      2
2
3
4
5
6
7
8
9
10  round
11   oval
12   long
pattern  light      0
pattern  light      3
  plain  light      1
  plain  light      3
  plain  light      2
pattern   dark      9
pattern   dark      0
pattern   dark      2
  plain   dark      1
  plain   dark     11
  plain   dark      2
oval
 long
round
 oval
 long
round
 oval
 long
We now have a data structure that contains the entire data set. This is a significant improvement over having four separate vectors because it properly represents the fact that the first value in each vector corresponds to information about the same type of candy.
An important feature of data frames is the fact that each column within a data frame can contain a different data type. For example, in the candy data frame, the first three columns contain text and the last column is numeric. However, all columns of a data frame must have the same length.
Data frames are displayed in a tabular layout, with column names above and row numbers to the left.
Another detail to notice about the way that data frames are displayed is that the text values in the first three columns do not have double-quotes around them (compare this with the display of text in character vectors in Section 9.4.2). Although character values must always be surrounded by double-quotes when we write code, they are not always displayed with double-quotes.
9.4.5 Lists
Vectors, factors, and data frames are the typical data structures that we create to represent our data values in computer memory. However, several other basic data structures are also important because when we call a function, the result could be any sort of data structure. We need to understand and be able to work with a variety of data structures.

As an example, consider the result of the following code:
           > dimnames(candy)
           [[1]]
            [1] "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "9"  "10" "11"
[12] "12"
           [[2]]
           [1] "shapes"   "patterns" "shades"   "counts"
The dimnames() function extracts the column names and the row names from the candy data frame; these are the values that are displayed above and to the left of the data frame (see the example in the previous section). There are 4 columns and 12 rows, so the dimnames() function has to return two character vectors that have different lengths. A data frame can contain two vectors, but the vectors cannot have different lengths; the only way the dimnames() function can return these values is as a list.
In this case, we have a list with two components. The first component is a character vector containing the 12 row names and the second component is another character vector containing the 4 column names.
Notice the way that lists are displayed. The first component of the list starts with the component index, [[1]], followed by the contents of this component, which is a character vector containing the names of the rows from the data frame.
           [[1]]
            [1] "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "9"  "10" "11"
[12] "12"
The second component of the list starts with the component index [[2]], followed by the contents of this component, which is also a character vector, this time the column names.
           [[2]]
           [1] "shapes"   "patterns" "shades"   "counts"
The list() function can be used to create a list explicitly. Like the c() function, list() takes any number of arguments; if the arguments are named, those names are used for the components of the list.

In the following code, we generate a list similar to the previous one, containing the row and column names from the candy data frame.
           > list(rownames=rownames(candy),
                   colnames=colnames(candy))
           $rownames
            [1] "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "9"  "10" "11"
[12] "12"
           $colnames
           [1] "shapes"   "patterns" "shades"   "counts"
The difference is that, instead of calling dimnames() to get the entire list, we have called rownames() to get the row names as a character vector, colnames() to get the column names as another character vector, and then list() to combine the two character vectors into a list. The advantage is that we have been able to provide names for the components of the list. These names are evident in how the list is displayed on the screen.
A list is a very flexible data structure. It can have any number of components, each of which can be any data structure of any length or size. A simple example is a data-frame-like structure where each column can have a different length, but much more complex structures are also possible. For example, it is possible for a component of a list to be another list.
Anyone who has worked with a computer should be familiar with the idea of a list containing another list because a directory or folder of files has this sort of structure: a folder contains multiple files of different kinds and sizes and a folder can contain other folders, which can contain more files or even more folders, and so on. Lists allow for this kind of hierarchical structure.
9.4.6 Matrices and arrays
Another sort of data structure in R, which lies in between vectors and data frames, is the matrix. This is a two-dimensional structure (like a data frame), but one where all values are of the same type (like a vector).
As for lists, it is useful to know how to work with matrices because many R functions either return a matrix as their result or take a matrix as an argument.
A matrix can be created directly using the matrix() function. The following code creates a matrix from 6 values, with 3 columns and two rows; the values are used column-first.
           > matrix(1:6, ncol=3)
                [,1] [,2] [,3]
           [1,]    1    3    5
           [2,]    2    4    6
The array data structure extends the idea of a matrix to more than two dimensions. For example, a three-dimensional array corresponds to a data cube.
The array() function can be used to create an array. In the following code, a two-by-two-by-two, three-dimensional array is created.
> array(1:8, dim=c(2, 2, 2)) ,,1
                [,1] [,2]
           [1,]    1    3
           [2,]    2    4
,,2
                [,1] [,2]
           [1,]    5    7
           [2,]    6    8
9.4.7 Flashback: Numbers in computer memory
Although we are not generally concerned with the bit-level or byte-level details of how data values are stored by R in RAM, we do need to be aware of one of the issues that was raised in Section 5.3.1.
In that section, we discussed the fact that there are limits to the precision with which numeric values can be represented in computer memory. This is true of numeric values stored in RAM just as it was for numeric values stored on some form of mass storage.
As a simple demonstration, consider the following condition.

           > 0.3 - 0.2 == 0.1
[1] FALSE
That apparently incorrect result is occurring because it is not actually possible to store an apparently simple value like 0.1 with absolute precision in computer memory (using a binary representation). The stored value is very, very close to 0.1, but it is not exact. In the condition above, the bit-level representations of the two values being compared are not identical, so the values, in computer memory, are not strictly equal.
Comparisons between real values must be performed with care and tests for equality between real values are not considered to be meaningful. The function all.equal() is useful for determining whether two real values are (approximately) equivalent.
Another issue is the precision to which numbers are displayed. Consider the following simple arithmetic expression.
> 1/3
           [1] 0.3333333
The answer, to full precision, never ends, but R has only shown seven significant digits. There is a limit to how many decimal places R could display because of the limits of representing numeric values in memory, but there is also a global option that controls (approximately) how many digits that R will display.
The following code uses the options() function to specify that R should display more significant digits.
           > options(digits=16)
           > 1/3
           [1] 0.3333333333333333
The code below sets the option back to its default value.
           > options(digits=7)

Recap
A vector is a one-dimensional data structure and all of its elements are of the same data type.
A factor is one-dimensional and every element must be one of a fixed set of values, called the levels of the factor.
A matrix is a two-dimensional data structure and all of its elements are of the same type.
A data frame is two-dimensional and different columns may contain different data types, though all values within a column must be of the same data type and all columns must have the same length.
A list is a hierarchical data structure and each component of a list may be any type of data structure whatsoever.
9.5 Subsetting
Now that we know some basic R functions that allow us to enter data values and we have a basic idea of how data values are represented in RAM, we are in a position to start working with the data values.
One of the most basic ways that we can manipulate data structures is to subset them—select a smaller portion from a larger data structure. This is analogous to performing a query on a database.
For example, we might want to answer the question: “what sort of candy was the most common in the bag of candy?” The following code produces the answer to this question using R’s subsetting capabilities.
           > candy[candy$counts == max(candy$counts), ]
              shapes patterns shades counts
           11   oval    plain   dark     11
R has very powerful mechanisms for subsetting. In this section, we will outline the basic format of these operations and many more examples will be demonstrated as we progress through the rest of the chapter.
We will start with subsetting vectors.
A subset from a vector may be obtained by appending an index within square brackets to the end of a symbol name. As an example, recall the vector of candy counts, called counts.
           > counts
            [1]  2  0  3  1  3  2  9  0  2  1 11  2
We can extract fourth of these counts by specifying the index 4, as shown below.
> counts[4]
[1] 1
The components of this expression are shown below:
symbol: counts[4] square brackets: counts[4] index: counts[4]
The index can be a vector of any length. For example, the following code produces the first three counts (the number of light-shaded candies with a pattern).
           > counts[1:3]
[1] 2 0 3
The diagram below illustrates how the values in the index vector are used to select elements from the counts vector to create a subset. Diagrams like this will be used throughout this chapter to help describe different data manipulation techniques; in each case, to save on space, the data values shown in the diagram will not necessarily correspond to the data values being discussed in the surrounding text.

The index does not have to be a contiguous sequence, and it can include repetitions. The following example produces counts for all candies with a pattern. The elements we want are the first three and the seventh, eighth, and ninth. The index is constructed with the following code.
> c(1:3, 7:9)
[1] 1 2 3 7 8 9
The subsetting operation is expressed as follows.
           > counts[c(1:3, 7:9)]
[1] 2 0 3 9 0 2
This is an example of a slightly more complex R expression. It involves a function call, to the c() function, that generates a vector, and this vector is then used as an index to select the corresponding elements from the counts vector.

The components of this expression are shown below:
symbol: square brackets: index:
counts[c(1:3, 7:9)]
counts[c(1:3, 7:9)]
counts[c(1:3, 7:9)]
As well as using integers for indices, we can use logical values. For example, a better way to express the idea that we want the counts for all candies with a pattern is to generate a logical vector first, as in the following code.
> hasPattern <- patterns == "pattern"
> hasPattern
 [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE
[10] FALSE FALSE FALSE
This logical vector can be used as an index to return all of the counts where hasPattern is TRUE.
> counts[hasPattern]
[1] 2 0 3 9 0 2
The diagram below illustrates how an index of logical values selects elements from the complete object where the index value is TRUE.

It would be even better to work with the entire data frame and retain the pattern with the counts, so that we can see that we have the correct result. We will now look at how subsetting works for two-dimensional data structures such as data frames.
A data frame can also be indexed using square brackets, though slightly differently because we have to specify both which rows and which columns we want. The following code extracts the patterns and counts variables, columns 2 and 4, from the data frame for all candies with a pattern:
           > candy[hasPattern, c(2, 4)]
             patterns counts
           1  pattern      2
           2  pattern      0
           3  pattern      3
           7  pattern      9
           8  pattern      0
           9  pattern      2
The result is still a data frame, just a smaller one. The components of this expression are shown below:
symbol: square brackets: row index: comma: column index:
candy[hasPattern, c(2, 4)]
count[hasPattern, c(2, 4)]
count[hasPattern, c(2, 4)]
count[hasPattern, c(2, 4)]
count[hasPattern, c(2, 4)]
An even better way to select this subset is to refer to the appropriate columns by their names. When a data structure has named components, a subset may be selected using those names. For example, the previous subset could also be obtained with the following code.

           > candy[hasPattern, c("patterns", "counts")]
             patterns counts
           1  pattern      2
           2  pattern      0
           3  pattern      3
           7  pattern      9
           8  pattern      0
           9  pattern      2
The function subset() provides another way to subset a data frame. This function has a subset argument for specifying the rows and a select argument for specifying the columns.
           > subset(candy, subset=hasPattern,
                     select=c("patterns", "counts"))
             patterns counts
           1  pattern      2
           2  pattern      0
           3  pattern      3
           7  pattern      9
           8  pattern      0
           9  pattern      2
When subsetting using square brackets, it is possible to leave the row or column index completely empty. The result is that all rows or all columns, respectively, are returned. For example, the following code extracts all columns for the first three rows of the data frame (the light-shaded candies with a pattern).
           > candy[1:3, ]
             shapes patterns shades counts
           1  round  pattern  light      2
           2   oval  pattern  light      0
           3   long  pattern  light      3
If a single index is specified when subsetting a data frame with single square brackets, the effect is to extract the appropriate columns of the data frame and all rows are returned.
           > candy["counts"]
counts 12 20 33 41 53 62 79 80 92 10 1 11 11 12 2
This result is one that we need to study more closely. This subsetting operation has extracted a single variable from a data frame. However, the result is a data frame containing a single column (i.e., a data set with one variable).
Often what we will require is just the vector representing the values in the variable. This is achieved using a different sort of indexing that uses double square brackets, [[. For example, the following code extracts the first variable from the candy data frame as a vector.
           > candy[["counts"]]
            [1]  2  0  3  1  3  2  9  0  2  1 11  2
The components of this expression are shown below:
symbol: candy[["counts"]] double square brackets: candy[["counts"]] index: candy[["counts"]]
Single square bracket subsetting on a data frame is like taking an egg container that contains a dozen eggs and chopping up the container so that we are left with a smaller egg container that contains just a few eggs. Double square bracket subsetting on a data frame is like selecting just one egg from an egg container.

As with single square bracket subsetting, the index used for double square bracket subsetting can also be a number.
           > candy[[4]]
            [1]  2  0  3  1  3  2  9  0  2  1 11  2
However, with double square bracket subsetting, the index must be a single value.
There is also a short-hand equivalent for getting a single variable from a data frame. This involves appending a dollar sign, $, to the symbol, followed by the name of the variable.
           > candy$counts
            [1]  2  0  3  1  3  2  9  0  2  1 11  2
The components of this expression are shown below:
symbol: candy$counts dollar sign: candy$counts variable name: candy$counts
9.5.1 Assigning to a subset
The subsetting syntax can also be used to assign a new value to some portion of a larger data structure. As an example, we will look at replacing the zero values in the counts vector (the counts of candies) with a missing value, NA.
As with extracting subsets, the index can be a numeric vector, a character vector, or a logical vector. In this case, we will first develop an expression that generates a logical vector telling us where the zeroes are.
> counts == 0
            [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE
           [10] FALSE FALSE FALSE
The zeroes are the second and eighth values in the vector.
We can now use this expression as an index to specify which elements of the counts vector we want to modify. > counts[counts == 0] <- NA
           > counts
            [1]  2 NA  3  1  3  2  9 NA  2  1 11  2
We have replaced the original zero values with NAs.
The following code reverses the process and replaces all NA values with zero. The is.na() function is used to find which values within the counts vector are NAs.
           > counts[is.na(counts)] <- 0
9.5.2 Subsetting factors
The case of subsetting a factor deserves special mention because, when we subset a factor, the levels of the factor are not altered. For example, consider the patterns variable in the candy data set.
           > candy$patterns
            [1] pattern pattern pattern plain   plain   plain   pattern
            [8] pattern pattern plain   plain   plain
           Levels: pattern plain
This factor has two levels, pattern and plain. If we subset just the first three values from this factor, the result only contains the value pattern, but there are still two levels.
           > candy$patterns[1:3]
           [1] pattern pattern pattern
           Levels: pattern plain
It is possible to force the unused levels to be dropped by specifying drop=TRUE within the square brackets, as shown below.

           > subPattern <- candy$patterns[1:3, drop=TRUE]
           > subPattern
           [1] pattern pattern pattern
           Levels: pattern
Assigning values to a subset of a factor is also a special case because only the current levels of the factor are allowed. A missing value is generated if the new value is not one of the current factor levels (and a warning is displayed).
For example, in the following code, we attempt to assign a new value to the first element of a factor where the new value is not one of the levels of the factor, so the result is an NA value.
           > subPattern[1] <- "swirly"
           > subPattern
           [1] <NA>    pattern pattern
           Levels: pattern
Recap
Single square brackets, [ ], select one or more elements from a data structure. The result is the same sort of data structure as the original object, just smaller.
The index within the square brackets can be a numeric vector, a logical vector, or a character vector.
Double square brackets, [[ ]], select a single element from a data structure. The result is usually a simpler data structure than the original.
The dollar sign, $, is short-hand for double square brackets.
9.6 More on data structures
This section provides some more details about how R data structures work. The information in this section is a little more advanced, but most of it is still useful for everyday use of R and most of it will be necessary to completely understand some of the R functions that are introduced in later sections.

9.6.1 The recycling rule
R allows us to work with vectors of values, rather than with single values, one at a time. This is very useful, but it does raise the issue of what to do when vectors have different lengths.
There is a general, but informal, rule in R that, in such cases, the shorter vector is recycled to become the same length as the longer vector. This is easiest to demonstrate via simple arithmetic.
In the following code, a vector of length 3 is added to a vector of length 6.
                 > c(1, 2, 3) + c(1, 2, 3, 4, 5, 6)
[1] 2 4 6 5 7 9
What happens is that the first vector is recycled to make a vector of length 6, and then element-wise addition can occur.
This rule is not necessarily followed in all possible situations, but it is the expected behavior in most cases.

9.6.2 Type coercion
In the case study in Section 9.1, there was a step where we took the text representation of a world population estimate and converted it into a number. This step is repeated below, broken down into a little more detail.
We start with a character vector (containing just one character value).
> popText
[1] "6,617,746,521"
We remove the commas from the text, but we still have a character vector.
                 > popNum <- gsub(",", "", popText)
                 > popNum
                 [1] "6617746521"
Now, we convert the character vector to a numeric vector.
                 > pop <- as.numeric(popNum)
                 > pop
[1] 6617746521
The important part is the call to as.numeric(). This is the function that
starts with a character value and converts it into a numeric value.
This process is called type coercion and it is important because we need the data in different forms to be able to perform different tasks. In this example, we need the data, which is an estimate of the world population, as a number so that we can subtract it from another, later, estimate of the world population. We cannot do the subtraction on character values.
There are many functions of the form as.type() for deliberately converting between different data structures like this. For example, the function for converting data into a character vector is called as.character().
It is also important to keep in mind that many functions will automatically perform type coercion if we give them an argument in the wrong form. To demonstrate this, we will consider the shapes variable in the candy data frame.

The shapes vector that we created first is a character vector. > shapes
            [1] "round" "oval"  "long"  "round" "oval"  "long"  "round"
            [8] "oval"  "long"  "round" "oval"  "long"
We used this character vector, plus several others, to create the candy data frame, with a call to the data.frame() function.
> candy <- data.frame(shapes, patterns, shades, counts)
What do we see if we extract the shapes column from the candy data frame?
           > candy$shapes
            [1] round oval  long  round oval  long  round oval  long
           [10] round oval  long
           Levels: long oval round
This is a factor, not a character vector!
How did the original character vector become a factor within the data frame? The data.frame() function automatically performed this type coercion (without telling us!).
This sort of automatic coercion happens a lot in R. Often, it is very convenient, but it is important to be aware that it may be happening, it is important to notice when it happens, and it is important to know how to stop it from happening. In some cases it will be more appropriate to perform type coercion explicitly, with functions such as as.numeric() and as.character(), and in some cases functions will provide arguments that turn the coercion off. For example, the data.frame() function provides a stringsAsFactors argument that controls whether character data are automatically converted to a factor.
Coercing factors
As we saw with subsetting factors, performing type coercion requires special care when we are coercing from a factor to another data type.
The correct sequence for coercing a factor is to first coerce it to a character vector and then to coerce the character vector to something else.
In particular, when coercing a factor that has levels consisting entirely of digits, the temptation is to call as.numeric() directly. However, the correct approach is to call as.character() and then as.numeric().

9.6.3 Attributes
Consider the candy data frame again. > candy
              shapes patterns shades counts
           1   round  pattern  light      2
2
3
4
5
6
7
8
9
10  round
11   oval
12   long
pattern  light      0
pattern  light      3
  plain  light      1
  plain  light      3
  plain  light      2
pattern   dark      9
pattern   dark      0
pattern   dark      2
  plain   dark      1
  plain   dark     11
  plain   dark      2
oval
 long
round
 oval
 long
round
 oval
 long
This data set consists of four variables that record how many candies were counted for each combination of shape, pattern, and shade. These variables are stored in the data frame as three factors and a numeric vector.
The data set also consists of some additional metadata. For example, each variable has a name. How is this information stored as part of the data frame data structure?
The answer is that the column names are stored as attributes of the data frame.
Any data structure in R may have additional information attached to it as an attribute; attributes are like an extra list structure attached to the primary data structure. The diagram below illustrates the idea of a data frame, on the left, with a list of attributes containing row names and column names, on the right.

In this case, the data frame has an attribute containing the row names and another attribute containing the column names.
As another example, consider the factor shapesFactor that we created on page 257.
> shapesFactor
 [1] round oval  long  round oval  long  round oval  long
[10] round oval  long
Levels: round oval long
Again, there are the actual data values, plus there is metadata that records the set of valid data values—the levels of the factor. This levels information is stored in an attribute. The diagram below illustrates the idea of a factor, on the left, with a single attribute that contains the valid levels of the factor, on the right.
We do not usually work directly with the attributes of factors or data frames.

However, some R functions produce a result that has important information stored in the attributes of a data structure, so it is necessary to be able to work with the attributes of a data structure.
The attributes() function can be used to view all attributes of a data structure. For example, the following code extracts the attributes from the candy data frame (we will talk about the class attribute later).
           > attributes(candy)
           $names
           [1] "shapes"   "patterns" "shades"   "counts"
           $row.names
            [1]  1  2  3  4  5  6  7  8  9 10 11 12
           $class
           [1] "data.frame"
This is another result that we should look at very closely. What sort of data structure is it?
This is a list of attributes.
Each component of this list, each attribute of the data frame, has a name; the column names of the candy data frame are in a component of this attribute list called names and the row names of the candy data frame are in a component of this list called row.names.
The attr() function can be used to get just one specific attribute from a data structure. For example, the following code just gets the names attribute from the candy data frame.
           > attr(candy, "names")
           [1] "shapes"   "patterns" "shades"   "counts"
Because many different data structures have a names attribute, there is a special function, names(), for extracting just that attribute.
           > names(candy)
           [1] "shapes"   "patterns" "shades"   "counts"

For the case of a data frame, we could also use the colnames() function to get this attribute, and there is a rownames() function for obtaining the row names.
Similarly, there is a special function, levels(), for obtaining the levels of a factor.
           > levels(shapesFactor)
           [1] "round" "oval"  "long"
Section 9.9.3 contains an example where it is necessary to directly access the attributes of a data structure.
9.6.4 Classes
We can often get some idea of what sort of data structure we are working with by simply viewing how the data values are displayed on screen. However, a more definitive answer can be obtained by calling the class() function.
For example, the data structure that has been assigned to the symbol candy is a data frame.
> class(candy)
[1] "data.frame"
The shapes variable within the candy data frame is a factor.
           > class(candy$shapes)
[1] "factor"
Many R functions return a data structure that is not one of the basic data structures that we have already seen. For example, consider the following code, which generates a table of counts of the number of candies of each different shape (summing across shades and patterns).
We will describe the xtabs() function later in Section 9.8.4. For now, we are just interested in the data structure that is produced by this function.

           > shapeCount <- xtabs(counts ~ shapes, data=candy)
           > shapeCount
           shapes
            long  oval round
9 14 13
What sort of data structure is this? The best way to find out is to use the class() function.

           > class(shapeCount)
           [1] "xtabs" "table"
The result is an "xtabs" data structure, which is a special sort of "table" data structure.
We have not seen either of these data structures before. However, much of what we already know about working with the standard data structures, and some of what we will learn in later sections, will also work with any new class that we encounter.
For example, it is usually possible to subset any class using the standard square bracket syntax. For example, in the following code, we extract the first element from the table.
           > shapeCount[1]
long 9
Where appropriate, arithmetic and comparisons will also generally work. In the code below, we are calculating which elements of the table are greater than 10.
           > shapeCount > 10
           shapes
            long  oval round
           FALSE  TRUE  TRUE
Furthermore, if necessary, we can often resort to coercing a class to something more standard and familiar. The following code converts the table data structure into a data frame, where the rows of the original table have been converted into columns in the data frame, with appropriate column names automatically provided.
           > as.data.frame(shapeCount)
             shapes Freq
           1   long    9
           2   oval   14
           3  round   13

In summary, although we will encounter a wide variety of data structures, the standard techniques that we learn in this chapter for working with basic data structures, such as subsetting, will also be useful for working with other classes.
9.6.5 Dates
Dates are an important example of a special data structure. Representing dates as just text is convenient for humans to view, but other representations are better for computers to work with.
As an example, we will make use of the Sys.Date() function, which returns the current date.
           > today <- Sys.Date()
           > today
           [1] "2008-09-29"
This looks like it is a character vector, but it is not. It is a Date data structure.
           > class(today)
[1] "Date"
Having a special class for dates means that we can perform tasks with dates, such as arithmetic and comparisons, in a meaningful way, something we could not do if we stored the date as just a character value. For example, the manuscript for this book was due at the publisher on September 30th 2008. The following code calculates whether the manuscript was late. The as.Date() function converts a character value, in this case "2008-09-30", into a date.
           > deadline <- as.Date("2008-09-30")
           > today > deadline
[1] FALSE
The following code calculates how many days remain before the deadline (or how late the manuscript is).

           > deadline - today
           Time difference of 1 days
The Date class stores date values as integer values, representing the number of days since January 1st 1970, and automatically converts the numbers to a readable text value to display the dates on the screen.
9.6.6 Formulas
Another important example of a special class in R is the formula class.
A formula is created by the special tilde operator, ~. Formulas are created
to describe relationships using symbols.
We saw an example on page 281 that looked like this:
           > counts ~ shapes
           counts ~ shapes
The components of this expression are show below:
left-hand side:
tilde operator: right-hand side:
counts ~ shapes
counts ~ shapes
counts ~ shapes
The result of a formula is just the formula expression itself. A formula only involves the symbol names; any data values that have been assigned to the symbols in the formula are not accessed at all in the creation of the formula.
Each side of a formula can have more than one symbol, with the symbols separated by standard operators such as + and *.
For example, the following call to the xtabs() function combines two symbols on the right-hand side of the formula to produce a two-way table of counts.
> xtabs(counts ~ patterns + shades, data=candy)
         shades
patterns  dark light
  pattern   11     5
  plain     14     6

Formulas are mainly used to express statistical models in R, but they are also used in other contexts, such as the xtabs() function shown above and in Section 9.6.4. We will see another use of formulas in Section 9.8.11.
9.6.7 Exploring objects
When working with anything but tiny data sets, basic features of the data set cannot be determined by just viewing the data values. This section describes a number of functions that are useful for obtaining useful summary features from a data structure.
The summary() function produces summary information for a data structure. For example, it will provide numerical summary data for each variable in a data frame.
> summary(candy)
   shapes     patterns   shades
 long :4   pattern:6   dark :6
 oval :4   plain  :6   light:6
 round:4
counts Min.:0 1st Qu.: 1 Median : 2 Mean :3 3rd Qu.: 3 Max. :11
The length() function is useful for determining the number of values in a vector or the number of components in a list. Similarly, the dim() function will give the number of rows and columns in a matrix or data frame.
> dim(candy)
[1]12 4
The str() function (short for “structure”) is useful when dealing with large objects because it only shows a sample of the values in each part of the object, although the display is very low-level so it may not always make things clearer.
The following code displays the low-level structure of the candy data frame. > str(candy)

'data.frame': 12 obs. of 4 variables:
$ shapes : Factor w/ 3 levels "long","oval",..: 3 2 1 3 .. $ patterns: Factor w/ 2 levels "pattern","plain": 1 1 1 2.. $ shades : Factor w/ 2 levels "dark","light": 2 2 2 2 2 .. $counts :num 2031329021...
Another function that is useful for inspecting a large object is the head() function. This just shows the first few elements of an object, so we can see the basic structure without seeing all of the values. The code below uses head() to display only the first six rows of the candy data frame.
           > head(candy)
             shapes patterns shades counts
           1  round  pattern  light      2
           2   oval  pattern  light      0
           3   long  pattern  light      3
4  round
5   oval
6   long
plain  light      1
plain  light      3
plain  light      2
There is also a tail() function for viewing the last few elements of an object.
9.6.8 Generic functions
In Section 9.6.2, we saw that some functions automatically perform type coercion. An example is the paste() function, which combines text values. If we give it a value that is not text, it will automatically coerce it to text. For example, the following code returns a number (the total number of candies in the candy data frame).
> sum(candy$counts)
[1] 36
If we use this value as an argument to the paste() function, the number is automatically coerced to a text value to become part of the overall text result.

           > paste("There are", sum(candy$counts), "long candies")
           [1] "There are 36 long candies"
Generic functions are similar in that they will accept many different data structures as arguments. However, instead of forcing the argument to be what the function wants it to be, a generic function adapts itself to the data structure it is given. Generic functions do different things when given different data structures.
An example of a generic function is the summary() function. The result of a call to summary() will depend on what sort of data structure we provide. The summary information for a factor is a table giving how many times each level of the factor appears.
           > summary(candy$shapes)
long oval round 444
If we provide a numeric vector, the result is a five-number summary, plus the mean.
           > summary(candy$counts)
Min. 1st Qu. Median Mean 3rd Qu. Max. 0 1 2 3 3 11
Generic functions are another reason why it is easy to work with data in R; a single function will produce a sensible result no matter what data structure we provide.
However, generic functions are also another reason why it is so important to be aware of what data structure we are working with. Without knowing what sort of data we are using, we cannot know what sort of result to expect from a generic function.
Recap
When two vectors of different lengths are used together, the shorter vector is often recycled to make it the same length as the longer vector.
Type coercion is the conversion of data values from one data type or data structure to another. This may happen automatically within a function, so care should be taken to make sure that data values returned by a function have the expected data type or data structure.
Any data structure may have attributes, which provide additional information about the data structure. These are in the form of a list of data values that are attached to the main data structure.
All data structures have a class.
Basic data manipulations such as subsetting, arithmetic, and comparisons, should still work even with unfamiliar classes.
Dates and formulas are important examples of special classes.
A generic function is a function that produces a different result depending on the class of its arguments.

9.7 Data import/export
Almost all of the examples so far have used data that are typed explicitly as R expressions. In practice, data usually reside in one or more files of various formats and in some form of mass storage. This section looks at R functions that can be used to read data into R from such external files.
We will look at functions that deal with all of the different data storage options that were discussed in Chapter 5: plain text files, XML documents, binary files, spreadsheets, and databases.
We will also look at some functions that go the other way and write a data structure from RAM to external mass storage.
9.7.1 The working directory
This section provides a little more information about how the R software environment works, with respect to reading and writing files.
Any files created during an R session are created in the current working directory of the session, unless an explicit path or folder or directory is specified. Similarly, when files are read into an R session, they are read from the current working directory.
On Linux, the working directory is the directory that the R session was started in. This means that the standard way to work on Linux is to create a directory for a particular project, put any relevant files in that directory, change into that directory, and then start an R session.
On Windows, it is typical to start R by double-clicking a shortcut or by selecting from the list of programs in the ‘Start’ menu. This approach will, by default, set the working directory to one of the directories where R was installed, which is a bad place to work. Instead, it is a good idea to work in a separate directory for each project, create a shortcut to R within that directory, and set the ‘Start in’ field on the properties dialog of the shortcut to be the directory for that project. An alternative is to use the setwd() function or the ‘Change dir’ option on the ‘File’ menu to explicitly change the working directory to something appropriate when working on a particular project.
9.7.2 Specifying files
In order to read or write a file, the first thing we need to be able to do is specify which file we want to work with. Any function that works with a file requires a precise description of the name of the file and the location of the file.
A filename is just a character value, e.g., "pointnemotemp.txt", but specifying the location of a file can involve a path, which describes a location on a persistent storage medium, such as a hard drive.
The best way to specify a path in R is via the file.path() function because this avoids the differences between path descriptions on different operating systems. For example, the following code generates a path to the file pointnemotemp.txt within the directory LAS (on a Linux system).
           > file.path("LAS", "pointnemotemp.txt")
           [1] "LAS/pointnemotemp.txt"
The file.choose() function can be used to allow interactive selection of a file. This is particularly effective on Windows because it provides a familiar file selection dialog box.
9.7.3 Text formats
R has functions for reading in the standard types of plain text formats: delimited formats, especially CSV files, and fixed-width formats (see Section 5.2). We will briefly describe the most important functions and then demonstrate their use in an example.
The read.table() function works for data in a delimited format. By default, the delimiter is whitespace (spaces and tabs), but an alternative may be specified via the sep argument. There is a read.csv() function for the special case of CSV files, and for data in a fixed-width format, there is the read.fwf() function.
The important arguments to these functions are the name of the external file and information about the format of the file. For example, in order to read a file in a fixed-width format with the read.fwf() function, we have to supply the widths of the fields in the file via the widths argument.
The result returned by all of these functions is a data frame.
Another important piece of information required when reading text files is the data type of each column of data in the file. Everything in the file is text, including numeric values, which are stored as a series of digits, and this means that some of the text values from the file may need to be coerced so that they are stored as an appropriate data type in the resulting data frame.
The general rule is that, if all values within a column in the text file are numbers, then the values in that column are coerced to a numeric vector. Otherwise, the values are used to create a factor. Several arguments are provided to control the coercion from the text values in the file to a specific data type; we will see examples in the next section and in other case studies throughout this chapter.
Another function that can be used to read text files is the readLines() function. The result in this case is a character vector, where each line of the text file becomes a separate element of the vector. The text values from the file become text values within RAM, so no type coercion is necessary in this case.
This function is useful for processing a file that contains text, but not in a standard plain text format. For example, the readLines() function was used to read the HTML code from the World Population Clock web site in Section 9.1. Section 9.9 discusses tools for processing data that have been read into R as text.
The following case study provides some demonstrations of the use of these functions for reading text files.

9.7.4 Case study: Point Nemo (continued)
The temperature data obtained from NASA’s Live Access Server for the Pacific Pole of Inaccessibility (see Section 1.1) were delivered in a plain text format (see Figure 9.5, which reproduces Figure 1.2 for convenience). How can we load this temperature information into R?
A read.table() example
One way to view the format of the file in Figure 9.5 is that the data start on the ninth line and data values within each row are separated by whitespace. This means that we can use the read.table() function to read the Point Nemo temperature information, as shown below.
                 > pointnemodelim <-
                        read.table(file.path("LAS", "pointnemotemp.txt"),
skip=8)
                 > pointnemodelim
                            V1 V2 V3 V4    V5
                 1 16-JAN-1994  0  / 1: 278.9
                 2 16-FEB-1994  0  / 2: 280.0
                 3 16-MAR-1994  0  / 3: 278.9
                 4 16-APR-1994  0  / 4: 278.9
                 5 16-MAY-1994  0  / 5: 277.8
                 6 16-JUN-1994  0  / 6: 276.1
...
In the above example, and in a number of examples throughout the rest of the chapter, the output displayed by R has been manually truncated to avoid wasting too much space on the page. This truncation is indicated by the use of ... at the end of a line of output, to indicate that there are further columns that are not shown, or ... on a line by itself, to indicate that there are further rows that are not shown.
By default, read.table() assumes that the text file contains a data set with one case on each row and that each row contains multiple values, with each value separated by whitespace (one or more spaces or tabs). The skip argument is used to ignore the first few lines of a file when, for example,

                         VARIABLE : Mean TS from clear sky composite (kelvin)
                         FILENAME : ISCCPMonthly_avg.nc
                         FILEPATH : /usr/local/fer_data/data/
                         SUBSET   : 48 points (TIME)
                         LONGITUDE: 123.8W(-123.8)
                         LATITUDE : 48.8S
123.8W 23
             16-JAN-1994 00 /  1:  278.9
             16-FEB-1994 00 /  2:  280.0
             16-MAR-1994 00 /  3:  278.9
             16-APR-1994 00 /  4:  278.9
             16-MAY-1994 00 /  5:  277.8
             16-JUN-1994 00 /  6:  276.1
             ...
Figure9.5: The first few lines of the plain text output from the LiveAccessServer for the surface temperature at Point Nemo. This is a reproduction of Figure 1.2.
there is header information or metadata at the start of the file before the core data values.
The result of this function call is a data frame, with a variable for each column of values in the text file. In this case, there are four instances of whitespace on each line, so each line is split into five separate values, resulting in a data frame with five columns.
The types of variables are determined automatically. If a column only contains numbers, the variable is numeric; otherwise, the variable is a factor.
The names of the variables in the data frame can be read from the file, or specified explicitly in the call to read.table(). Otherwise, as in this case, R will generate a unique name for each column: V1, V2, V3, etc.
The result in this case is not perfect because we end up with several columns of junk that we do not want (V2 to V4). We can use a few more arguments to read.table() to improve things greatly.
           > pointnemodelim <-
                  read.table(file.path("LAS", "pointnemotemp.txt"),
                             skip=8,
                             colClasses=c("character",
                                          "NULL", "NULL", "NULL",
                                          "numeric"),
                             col.names=c("date", "", "", "", "temp"))

           > pointnemodelim
                    date  temp
           1 16-JAN-1994 278.9
           2 16-FEB-1994 280.0
           3 16-MAR-1994 278.9
           4 16-APR-1994 278.9
           5 16-MAY-1994 277.8
           6 16-JUN-1994 276.1
...
The colClasses argument allows us to control the types of the variables explicitly. In this case, we have forced the first variable to be just text (these values are dates, not categories). There are five columns of values in the text file (treating whitespace as a column break), but we are not interested in the middle three, so we use "NULL" to indicate that these columns should just be ignored. The last column, the temperature values, is numeric.
It is common for the names of the variables to be included as the first line of a text file (the header argument can be used to read variable names from such a file). In this case, there is no line of column names in the file, so we provide the variable names explicitly, as a character vector, using the col.names argument.
The dates can be converted from character values to date values in a separate step using the as.Date() function.
           > pointnemodelim$date <- as.Date(pointnemodelim$date,
                                             format="%d-%b-%Y")
           > pointnemodelim
                   date  temp
           1 1994-01-16 278.9
           2 1994-02-16 280.0
           3 1994-03-16 278.9
           4 1994-04-16 278.9
           5 1994-05-16 277.8
           6 1994-06-16 276.1
...

The format argument contains special sequences that tell as.Date() where the various components of the date are within the character values. The %d means that there are two digits for the day, the %b means that the month is given as an abbreviated month name, and the %Y means that there are four digits for the year. The dashes are literal dash characters.
The way that these components map to the original character value for the first date is shown below.
two-digit day, %d: abbreviated month name, %b: four-digit year, %Y: literal dashes:
16-JAN-1994
16-JAN-1994
16-JAN-1994
16-JAN-1994
Thus, for example, the original character value 16-JAN-1994 becomes the date value 1994-01-16.
A read.fwf() example
Another way to view the Point Nemo text file in Figure 9.5 is as a fixed-width format file. For example, the date values always reside in the first 12 characters of each line and the temperature values are always between character 24 and character 28. This means that we could also read the file using read.fwf(), as shown below.
> pointnemofwf <-
       read.fwf(file.path("LAS", "pointnemotemp.txt"),
                skip=8,
                widths=c(-1, 11, -11, 5),
                colClasses=c("character", "numeric"),
                col.names=c("date", "temp"))
> pointnemofwf
         date  temp
1 16-JAN-1994 278.9
2 16-FEB-1994 280.0
3 16-MAR-1994 278.9
4 16-APR-1994 278.9
5 16-MAY-1994 277.8
6 16-JUN-1994 276.1
...

Again, the result is a data frame. As for the call to read.table(), we have specified the data type for each column, via the colClasses argument, and a name for each column, via col.names.
The widths argument specifies how wide each column of data is, with negative values used to ignore the specified number of characters. In this case, we have ignored the very first character on each line, we treat the next 11 characters as a date value, we ignore characters 13 to 23, and the final 5 characters are treated as the temp value.
The dates could be converted from character values to dates in exactly the same way as before.
A readLines() example
The two examples so far have demonstrated reading in the raw data for this data set, but so far we have completely ignored all of the metadata in the head of the file. This information is also very important and we would like to have some way to access it.
The readLines() function can help us here, at least in terms of getting raw text into R. The following code reads the first eight lines of the text file into a character vector.
           > readLines(file.path("LAS", "pointnemotemp.txt"),
                        n=8)
[1] "
[2] "
[3] "
[4] "
[5] "
[6] "
[7] "
[8] "
VARIABLE : Mean TS from clear sky composite (kelvin)"
FILENAME : ISCCPMonthly_avg.nc"
FILEPATH : /usr/local/fer_data/data/"
SUBSET   : 48 points (TIME)"
LONGITUDE: 123.8W(-123.8)"
LATITUDE : 48.8S"
123.8W " 23"
Section 9.9 will describe some tools that could be used to extract the metadata values from this text.

           date,temp
           1994-01-16,278.9
           1994-02-16,280
           1994-03-16,278.9
           1994-04-16,278.9
           1994-05-16,277.8
           1994-06-16,276.1
...
Figure 9.6: The first few lines of the plain text output from the Live Access Server for the surface temperature at Point Nemo in Comma-Separated Value (CSV) format. This is a reproduction of Figure 5.4.
A write.csv() example
As a simple demonstration of the use of functions that can write plain text files, we will now export the R data frame, pointnemodelim, to a new CSV file. This will create a much tidier plain text file that contains just the date and temperature values. Creating such a file is sometimes a necessary step in preparing a data set for analysis with a specific piece of analysis software.
The following code uses the write.csv() function to create a file called "pointnemoplain.csv" that contains the Point Nemo data (see Figure 9.6).
           > write.csv(pointnemodelim, "pointnemoplain.csv",
                        quote=FALSE, row.names=FALSE)
The first argument in this function call is the data frame of values. The second argument is the name of the file to create.
The quote argument controls whether quote-marks are printed around character values and the row.names argument controls whether an extra column of unique names is printed at the start of each line. In both cases, we have turned these features off by specifying the value FALSE.
We will continue to use the Point Nemo data set, in various formats, throughout the rest of this section.
9.7.5 Binary formats
As discussed in Section 5.3, it is only possible to extract data from a binary file format with an appropriate piece of software that understands the particular binary format.

A number of R packages exist for reading particular binary formats. For example, the foreign package contains functions for reading files produced by other popular statistics software systems, such as SAS, SPSS, Systat, Minitab, and Stata. As an example of support for a more general binary format, the ncdf package provides functions for reading netCDF files.
We will look again at the Point Nemo temperature data (see Section 1.1), this time in a netCDF format, to demonstrate a simple use of the ncdf package.
The following code loads the ncdf package and reads the file pointnemotemp.nc.
           > library("ncdf")
           > nemonc <- open.ncdf(file.path("LAS",
                                            "pointnemotemp.nc"))
One difference with this example compared to the functions in the previous section is that the result of reading the netCDF file is not a data frame.
           > class(nemonc)
[1] "ncdf"
This data structure is essentially a list that contains the information from the netCDF file. We can extract components of this list by hand, but a more convenient approach is to use several other functions provided by the ncdf package to extract the most important pieces of information from the ncdf data structure.
If we display the nemonc data structure, we can see that the file contains a single variable called Temperature.

> nemonc
           file LAS/pointnemotemp.nc has 1 dimensions:
           Time   Size: 48
           ------------------------
           file LAS/pointnemotemp.nc has 1 variables:
           double Temperature[Time]  Longname:Temperature
We can extract that variable from the file with the function get.var.ncdf(). > nemoTemps <- get.var.ncdf(nemonc, "Temperature")
> nemoTemps
            [1] 278.9 280.0 278.9 278.9 277.8 276.1 276.1 275.6 275.6
           [10] 277.3 276.7 278.9 281.6 281.1 280.0 278.9 277.8 276.7
           [19] 277.3 276.1 276.1 276.7 278.4 277.8 281.1 283.2 281.1
           [28] 279.5 278.4 276.7 276.1 275.6 275.6 276.1 277.3 278.9
           [37] 280.5 281.6 280.0 278.9 278.4 276.7 275.6 275.6 277.3
           [46] 276.7 278.4 279.5
The netCDF file of Point Nemo data also contains information about the date that each temperature value corresponds to. This variable is called "Time". The following code reads this information from the file.
           > nemoTimes <- get.var.ncdf(nemonc, "Time")
           > nemoTimes
            [1]  8781  8812  8840  8871  8901  8932  8962  8993  9024
           [10]  9054  9085  9115  9146  9177  9205  9236  9266  9297
           [19]  9327  9358  9389  9419  9450  9480  9511  9542  9571
           [28]  9602  9632  9663  9693  9724  9755  9785  9816  9846
           [37]  9877  9908  9936  9967  9997 10028 10058 10089 10120
           [46] 10150 10181 10211
Unfortunately, these do not look very much like dates.
This demonstrates that, even with binary file formats, it is often necessary to coerce data from the storage format that has been used in the file to a more convenient format for working with the data in RAM.
The netCDF format only allows numbers or text to be stored, so this date information has been stored in the file as numbers. However, additional information, metadata, can be stored along with the variable data in a netCDF file; netCDF calls this additional information “attributes”. In this case, the meaning of these numbers representing dates has been stored as the “units” attribute of this variable and the following code uses the att.get.ncdf() function to extract that attribute from the file.
           > att.get.ncdf(nemonc, "Time", "units")
           $hasatt
           [1] TRUE
           $value
           [1] "number of days since 1970-01-01"
This tells us that the dates were stored as a number of days since January 1st 1970. Using this information, we can convert the numbers from the file back into real dates with the as.Date() function. The origin argument allows us to specify the meaning of the numbers.
           > nemoDates <- as.Date(nemoTimes, origin="1970-01-01")
> nemoDates
            [1] "1994-01-16" "1994-02-16" "1994-03-16" "1994-04-16"
            [5] "1994-05-16" "1994-06-16" "1994-07-16" "1994-08-16"
            [9] "1994-09-16" "1994-10-16" "1994-11-16" "1994-12-16"
...
In most cases, where a function exists to read a particular binary format, there will also be a function to write data out in that format. For example, the ncdf package also provides functions to save data from RAM to an external file in netCDF format.
9.7.6 Spreadsheets
When data is stored in a spreadsheet, one common approach is to save the data in a text format as an intermediate step and then read the text file into R using the functions from Section 9.7.3.

This makes the data easy to share, because text formats are very portable, but it has the disadvantage that another copy of the data is created.
This is less efficient in terms of storage space and it creates issues if the original spreadsheet is updated.
If changes are made to the original spreadsheet, at best, there is extra work to be done to update the text file as well. At worst, the text file is forgotten and the update does not get propagated to other places.
There are several packages that provide ways to directly read data from a spreadsheet into R. One example is the (Windows only) xlsReadWrite package, which includes the read.xls() function for reading data from an Excel spreadsheet.

Figure 9.7 shows a screen shot of the Point Nemo temperature data (see Section 1.1) stored in a Microsoft Excel spreadsheet.
These data can be read into R using the following code. > library("xlsReadWrite")
           > read.xls("temperatures.xls", colNames=FALSE)
                V1    V2
           1 34350 278.9
           2 34381 280.0
           3 34409 278.9
           4 34440 278.9
           5 34470 277.8
           6 34501 276.1
...
Notice that the date information has come across as numbers. This is another example of the type coercion that can easily occur when transferring between different formats.

Figure 9.7: Part of the Excel spreadsheet containing the surface temperatures at Point Nemo.

As before, we can easily convert the numbers to dates if we know the reference date for these numbers. Excel represents dates as the number of days since the 0th of January 1900, so we can recover the real dates with the following code.
           > dates <- as.Date(temps$V1 - 2, origin="1900-01-01")
> dates
            [1] "1994-01-16" "1994-02-16" "1994-03-16" "1994-04-16"
            [5] "1994-05-16" "1994-06-16" "1994-07-16" "1994-08-16"
            [9] "1994-09-16" "1994-10-16" "1994-11-16" "1994-12-16"
...
We have to subtract 2 in this calculation because the Excel count starts from the 0th rather than the 1st of January and because Excel thinks that 1900 was a leap year (apparently to be compatible with the Lotus 123 spreadsheet software). Sometimes, computer technology is not straightforward.
The gdata package provides another way to access Excel spreadsheets with its own read.xls() function and it is also possible to access Excel spreadsheets via a technology called ODBC (see Section 9.7.8).
9.7.7 XML
In this section, we look at how to get information that has been stored in an XML document into R.
Although XML files are plain text files, functions like read.table() from Section 9.7.3 are of no use because they only work with data that are arranged in a plain text format, with the data laid out in rows and columns within the file.
It is possible to read an XML document into R as a character vector using a standard function like readLines(). However, extracting the information from the text is not trivial because it requires knowledge of XML.
Fortunately, there is an R package called XML that contains functions for reading and extracting data from XML files into R.
We will use the Point Nemo temperature data, in an XML format, to demonstrate some of the functions from the XML package. Figure 9.8 shows one possible XML format for the the Point Nemo temperature data.

            <?xml version="1.0"?>
            <temperatures>
                <variable>Mean TS from clear sky composite (kelvin)</variable>
                <filename>ISCCPMonthly_avg.nc</filename>
                <filepath>/usr/local/fer_dsets/data/</filepath>
                <subset>93 points (TIME)</subset>
                <longitude>123.8W(-123.8)</longitude>
                <latitude>48.8S</latitude>
                <case date="16-JAN-1994" temperature="278.9" />
                <case date="16-FEB-1994" temperature="280" />
                <case date="16-MAR-1994" temperature="278.9" />
                <case date="16-APR-1994" temperature="278.9" />
                <case date="16-MAY-1994" temperature="277.8" />
                <case date="16-JUN-1994" temperature="276.1" />
                ...
            </temperatures>
Figure 9.8: The first few lines of the surface temperature at Point Nemo in an XML format. This is a reproduction of Figure 5.16.

There are several approaches to working with XML documents in R using the XML package, but in this section, we will only consider the approach that allows us to use XPath queries (see Section 7.3.1).

The first thing to do is to read the XML document into R using the function xmlTreeParse().
           > library("XML")
           > nemoDoc <-
                  xmlParse(file.path("LAS",
                                     "pointnemotemp.xml"))
The main argument to this function is the name and location of the XML file.
It is important to point out that the data structure that is created in RAM by this code, nemoDoc, is not a data frame. > class(nemoDoc)
           [1] "XMLInternalDocument" "XMLAbstractDocument"
           [3] "oldClass"
We must use other special functions from the XML package to work with this data structure.
In particular, the getNodeSet() function allows us to select elements from this data structure using XPath expressions.
In the following example, we extract the temperature attribute values from all case elements in the XML document. The XPath expression "/temperatures/case/@temperature" selects all of the temperature attributes of the case elements within the root temperatures element.
           > nemoDocTempText <-
                  unlist(getNodeSet(nemoDoc,
                                    "/temperatures/case/@temperature"),
                         use.names=FALSE)
           > nemoDocTempText
            [1] "278.9" "280"   "278.9" "278.9" "277.8" "276.1" "276.1"
            [8] "275.6" "275.6" "277.3" "276.7" "278.9" "281.6" "281.1"
           [15] "280"   "278.9" "277.8" "276.7" "277.3" "276.1" "276.1"
           ...
The first argument to getNodeSet() is the data structure previously created by xmlParse() and the second argument is the XPath expression. The result of the call to getNodeSet() is a list and the call to unlist() converts the list to a vector.
One important point about the above result, nemoDocTempText, is that it is a character vector. This reflects the fact that everything is stored as text within an XML document. If we want to have numeric values to work with, we need to coerce these text values into numbers.
Before we do that, the following code shows how to extract the date values from the XML document as well. The only difference from the previous call is the XPath that we use.
           > nemoDocDateText <-
                  unlist(getNodeSet(nemoDoc,
                                    "/temperatures/case/@date"),
                         use.names=FALSE)
           > nemoDocDateText
            [1] "16-JAN-1994" "16-FEB-1994" "16-MAR-1994" "16-APR-1994"
            [5] "16-MAY-1994" "16-JUN-1994" "16-JUL-1994" "16-AUG-1994"
            [9] "16-SEP-1994" "16-OCT-1994" "16-NOV-1994" "16-DEC-1994"
...
Again, the values are all text, so we need to coerce them to dates. The following code performs the appropriate type coercions and combines the dates and temperatures into a data frame.
           > data.frame(date=as.Date(nemoDocDateText, "%d-%b-%Y"),
                         temp=as.numeric(nemoDocTempText))
                   date  temp
           1 1994-01-16 278.9
           2 1994-02-16 280.0
           3 1994-03-16 278.9
           4 1994-04-16 278.9
           5 1994-05-16 277.8
           6 1994-06-16 276.1
...
With this approach to reading XML files, there is one final step: we need to signal that we are finished with the file by calling the free() function.
           > free(nemoDoc)
9.7.8 Databases
Very large data sets are often stored in relational databases. As with spreadsheets, a simple approach to extracting information from the database is to export it from the database to text files and work with the text files. This is an even worse option for databases than it was for spreadsheets because it is more common to extract just part of a database, rather than an entire spreadsheet. This can lead to several different text files from a single database, and these are even harder to maintain if the database changes.
A superior option is to extract information directly from the database management system into R.
There are packages for connecting directly to several major database management systems. Two main approaches exist, one based on the DBI package and one based on the RODBC package.
The DBI package defines a set of standard (generic) functions for communicating with a database, and a number of other packages, e.g., RMySQL and RSQLite, build on that to provide functions specific to a particular database system. The important functions to know about with this approach are:
dbDriver()
to create a “device driver”, which contains information about a particular database management system.
           dbConnect(drv)

to create a “connection” to a database. Requires a device driver, drv, as created by dbDriver().
dbGetQuery(conn, statement)
to send the SQL command, statement, to the database and receive a result. The result is a data frame. Requires a connection, conn, as created by dbConnect().
           dbDisconnect(conn)
to sever the connection with the database and release resources.
The RODBC package defines functions for communicating with any ODBC (Open Database Connectivity) compliant software. This allows connections with many different types of software, including, but not limited to, most database management systems. For example, this approach can also be used to extract information from a Microsoft Excel spreadsheet.
The important functions to know about with this approach are:
           odbcConnect()
to connect to the ODBC application.
           sqlQuery(channel, query)
to send an SQL command to the database and receive a result, as a data frame. Requires a connection, channel, that has been created by odbcConnect().
           odbcClose(channel)
to sever the ODBC connection and release resources.
The RODBC approach makes it possible to connect to a wider range of other software systems, but it may involve installation of additional software.
The simplest approach of all is provided by the RSQLite package because it includes the complete SQLite application, so no other software needs to be installed. However, this will only be helpful if the data are stored in an SQLite database.
The next section demonstrates an example usage of the RSQLite package. 9.7.9 Case study: The Data Expo (continued)
The Data Expo data set (see Section 5.2.8) contains several different atmospheric measurements, all measured at 72 different time periods and 576 different locations. These data have been stored in an SQLite database, with a table for location information, a table for time period information, and a table of the atmospheric measurements (see Section 7.1).
The following SQL code extracts information for the first two locations in the location table.
           SELECT *
               FROM location_table
               WHERE ID = 1 OR ID = 2;
The following code carries out this query from within R. The first step is to connect to the database.
           > library("RSQLite")
           > con <- dbConnect(dbDriver("SQLite"),
                               dbname="NASA/dataexpo")
Having established the connection, we can send SQL code to the DBMS.
           > result <-
                  dbGetQuery(con,
> result
"SELECT *
     FROM location_table
     WHERE ID = 1 OR ID = 2")
  ID longitude latitude elevation
1  1   -113.75    36.25   1526.25
2  2   -111.25    36.25   1759.56
Notice that the result is a data frame. The final step is to release our connection to SQLite.
> dbDisconnect(con)
[1] TRUE
Recap
There are functions for reading plain text files in both delimited and fixed-width formats. These functions create a data frame from the data values in the file.

For many binary formats and spreadsheets, there exist packages with special functions for reading files in the relevant format.
The XML package provides special functions for working with XML documents.
Several packages provide special functions for extracting data from relational databases. The result of a query to a database is a data frame.
9.8 Data manipulation
This section describes a number of techniques for rearranging objects in R, particularly larger data structures, such as data frames, matrices, and lists.
Some of these techniques are useful for basic exploration of a data set. For example, we will look at functions for sorting data and for generating tables of counts.
Other techniques will allow us to perform more complex tasks such as restructuring data sets to convert them from one format to another, splitting data structures into smaller pieces, and combining smaller pieces to form larger data structures.
This section is all about starting with a data structure in RAM and calculating new values, or just rearranging the existing values, to generate a new data structure in RAM.
In order to provide a motivation for some of these techniques, we will use the following case study throughout this section.

9.8.1 Case study: New Zealand schools
The New Zealand Ministry of Education provides basic information for all primary and secondary schools in the country.2 In this case study, we will work with a subset of this information that contains the following variables:
ID
Name
The name of the school.
City
Auth
The“authority”or ownership of the school. Possible values are Private, State, State Integrated, and Other. A state integrated school is one that was private in the past, but is now state-owned; these schools usually have some special character (e.g., an emphasis on a particular religious belief).
Dec
Roll
These data have been stored in CSV format in a file called schools.csv (see Figure 9.9).
Using functions from the last section, we can easily read this information into R and store it as a data frame called schools. The as.is argument is used to ensure that the school names are kept as text and are not treated as a factor.
                 > schools <-
                        read.csv(file.path("Schools", "schools.csv"),
as.is=2)
2The data were originally obtained from http://www.minedu.govt.nz/web/ downloadable/dl6434_v1/directory-schools-web.xls and similar files are now available from http://www.educationcounts.govt.nz/statistics/tertiary_education/ 27436.
ii ii
A unique numeric identifier for each school.
The city where the school is located.
The “decile” of the school, which is a measure of the socio-economic status of the families of the students of the school. A lower decile roughly corresponds to a poorer school population. The value ranges from 1 (poorest) to 10 (richest).
The number of students enrolled at the school (as of July 2007).

           "ID","Name","City","Auth","Dec","Roll"
           1015,"Hora Hora School","Whangarei","State",2,318
           1052,"Morningside School","Whangarei","State",3,200
           1062,"Onerahi School","Whangarei","State",4,455
           1092,"Raurimu Avenue School","Whangarei","State",2,86
           1130,"Whangarei School","Whangarei","State",4,577
           1018,"Hurupaki School","Whangarei","State",8,329
           1029,"Kamo Intermediate","Whangarei","State",5,637
           1030,"Kamo School","Whangarei","State",5,395
           ...
Figure 9.9: The first few lines of the file schools.csv. This file contains information for all primary and secondary schools in New Zealand, in a CSV format.
> schools
               ID                  Name      City  Auth Dec Roll
                                                          2  318
                                                          3  200
                                                          4  455
                                                          2   86
                                                          4  577
                                                          8  329
In cases like this, where the data set is much too large to view all at once, we can make use of functions to explore the basic characteristics of the data set. For example, it is useful to know just how large the data set is. The following code tells us the number of rows and columns in the data frame.
           > dim(schools)
[1] 2571 6
There are 6 variables measured on 2,571 schools.
We will now go on to look at some data manipulation tasks that we could perform on this data set.
1 1015
2 1052
3 1062
4 1092 Raurimu Avenue School Whangarei State
5 1130      Whangarei School Whangarei State
6 1018       Hurupaki School Whangarei State
...
  Hora Hora School Whangarei State
Morningside School Whangarei State
    Onerahi School Whangarei State

9.8.2 Transformations
A common task in data preparation is to create new variables from an existing data set. As an example, we will generate a new variable that distinguishes between “large” schools and “small” schools.
This new variable will be based on the median school roll. Any school with a roll greater than the median will be “large”.
           > rollMed <- median(schools$Roll)
           > rollMed
[1] 193
We can use the ifelse() function to generate a character vector recording "large" and "small". The first argument to this function, called test, is a logical vector. The second argument, called yes, provides the result wherever the test argument is TRUE and the third argument, called no, provides the result wherever the test argument is FALSE.
           > size <- ifelse(test=schools$Roll > rollMed,
                             yes="large", no="small")
> size
            [1] "large" "large" "large" "small" "large" "large" "large"
            [8] "large" "large" "large" "large" "large" "small" "large"
           ...
The diagram below illustrates that the ifelse() function selects elements from the yes argument when the test argument is TRUE and from the no argument when test is FALSE.

We will now add this new variable to the original data frame to maintain the correspondence between the school size labels and the original rolls that they were derived from.
This can be done in several ways. The simplest is to assign a new variable to the data frame, as shown in the following code. The data frame does not have an existing column called Size, so this assignment adds a new column.
           > schools$Size <- size
> schools
                ID                  Name      City  Auth Dec Roll  Size
                                                           2  318 large
                                                           3  200 large
                                                           4  455 large
                                                           2   86 small
                                                           4  577 large
                                                           8  329 large
Another approach, which gives the same result, is to use the transform() function.
1 1015
2 1052
3 1062
4 1092 Raurimu Avenue School Whangarei State
5 1130      Whangarei School Whangarei State
6 1018       Hurupaki School Whangarei State
...
  Hora Hora School Whangarei State
Morningside School Whangarei State
    Onerahi School Whangarei State

           > schools <- transform(schools, Size=size)
If we want to remove a variable from a data frame, we can assign the value NULL to the appropriate column.
           > schools$size <- NULL
Alternatively, we can use subsetting to retain certain columns or leave out certain columns. This approach is better if more than one column needs to be removed. For example, the following code removes the new, seventh column from the data frame.
           > schools <- schools[, -7]

> schools
               ID                  Name      City  Auth Dec Roll
                                                          2  318
                                                          3  200
                                                          4  455
                                                          2   86
                                                          4  577
                                                          8  329
1 1015
2 1052
3 1062
4 1092 Raurimu Avenue School Whangarei State
5 1130      Whangarei School Whangarei State
6 1018       Hurupaki School Whangarei State
...
Binning
  Hora Hora School Whangarei State
Morningside School Whangarei State
    Onerahi School Whangarei State
The previous example converted a numeric variable into two categories, small and large. The more general case is to convert a numeric variable into any number of categories.
For example, rather than just dividing the schools into large and small, we could group the schools into five different size categories. This sort of transformation is possible using the cut() function.
> rollSize <-
       cut(schools$Roll, 5,
           labels=c("tiny", "small", "medium",
                    "large", "huge"))
The first argument to this function is the numeric vector to convert. The second argument says that the range of the numeric vector should be broken into five equal-sized intervals. The schools are then categorized according to which interval their roll lies within. The labels argument is used to provide levels for the resulting factor.
Only the first few schools are shown below and they are all tiny; the important point is that the result is a factor with five levels.
> head(rollSize)
[1] tiny tiny tiny tiny tiny tiny
Levels: tiny small medium large huge
A better view of the result can be obtained by counting how many schools there are of each size. This is what the following code does; we will see more about functions to create tables of counts in Section 9.8.4.
           > table(rollSize)
           rollSize
             tiny  small medium  large   huge
             2487     75      8      0      1
9.8.3 Sorting
Another common task that we can perform is to sort a set of values into ascending or descending order.
In R, the function sort() can be used to arrange a vector of values in order, but of more general use is the order() function, which returns the indices of the sorted values.
As an example, the information on New Zealand schools is roughly ordered by region, from North to South in the original file. We might want to order the schools by size instead.
The following code sorts the Roll variable by itself. > sort(schools$Roll)
[1] 5 5 6 6 6 7 7 7 7 8 9 9 9 9 9 9 9 ...
There are clearly some very small schools in New Zealand.
It is also easy to sort in decreasing order, which reveals that the largest school is the largest by quite some margin.
           > sort(schools$Roll, decreasing=TRUE)
            [1] 5546 3022 2613 2588 2476 2452 2417 2315 2266 2170 ...
However, what would be much more useful would be to know which schools these are. That is, we would like to sort not just the school rolls, but the entire schools data frame.

To do this, we use the order() function, as shown in the following code. > rollOrder <- order(schools$Roll, decreasing=TRUE)
> rollOrder
            [1] 1726  301  376 2307  615  199  467  373  389  241 ...
This result says that, in order to sort the data frame in descending roll order, we should use row 1726 first, then row 301, then row 376, and so on.
These values can be used as indices for subsetting the entire schools data frame, as shown in the following code. Recall that, by only specifying a row index and by leaving the column index blank, we will get all columns of the schools data frame.
           > schools[rollOrder, ]
      ID
1726 498
301   28
376   78
2307 319
615   41
199   43
467   54
                             Name
            Correspondence School
                Rangitoto College
                 Avondale College
...
                             City  Auth Dec Roll
                       Wellington State  NA 5546
                         Auckland State  10 3022
                         Auckland State   4 2613
Burnside High School Christchurch State   8 2588
                 Macleans College
               Massey High School
                 Auckland Grammar
373   69 Mt Albert Grammar School
389   74       Mt Roskill Grammar
Auckland State  10 2476
Auckland State   5 2452
Auckland State  10 2417
Auckland State   7 2315
Auckland State   4 2266
The largest body of New Zealand school students represents those gaining public education via correspondence (from the governmental base in Wellington), but most of the other large schools are in Auckland, which is the largest city in New Zealand.
The other advantage of using the order() function is that more than one vector of values may be given and any ties in the first vector will be broken by ordering on the second vector. For example, the following code sorts the rows of the schools data frame first by city and then by number of students (in decreasing order). In the case of the City variable, which is a character vector, the order is alphabetic. Again, we are specifying decreasing=TRUE to get descending order.
           > schools[order(schools$City, schools$Roll,
                            decreasing=TRUE), ]
                                           Name      City  Auth Dec Roll
                                Menzies College   Wyndham State   4  356
       ID
2548  401
2549 4054
1611 2742
1630 2640
2041 3600
                     Wyndham School   Wyndham State
                   Woodville School Woodville State
                    Papatawa School Woodville State
                     Woodend School   Woodend State
1601  399 Central Southland College    Winton State
...
5   94
3  147
7   27
9  375
7  549
The first two schools are both in Wyndham, with the larger school first and the smaller school second, then there are two schools from Woodville, larger first and smaller second, and so on.
9.8.4 Tables of counts
Continuing our exploration of the New Zealand schools data set, we might be interested in how many schools are private and how many are state-owned. This is an example where we want to obtain a table of counts for a categorical variable. The function table() may be used for this task.
> authTable <- table(schools$Auth)
> authTable
           Other          Private            State State Integrated
               1               99             2144              327
This result shows the number of times that each different value occurs in the Auth variable. As expected, most schools are public schools, owned by the state.
As usual, we should take notice of the data structure that this function has returned.
> class(authTable)
[1] "table"

This is not one of the basic data structures that we have focused on. However, tables in R behave very much like arrays, so we can use what we already know about working with arrays. For example, we can subset a table just like we subset an array. If we need to, we can also convert a table to an array, or even to a data frame.
As a brief side track, the table of counts above also shows that there is only one school in the category "Other". How do we find that school? One approach is to generate a logical vector and use that to subscript the data frame, as in the following code.
           > schools[schools$Auth == "Other", ]
                 ID            Name         City  Auth Dec Roll
           2315 518 Kingslea School Christchurch Other   1   51
It turns out that this school is not state-owned, but still receives its funding from the government because it provides education for students with learning or social difficulties.
Getting back to tables of counts, another question we might ask is how the school decile relates to the school ownership. If we give the table() function more than one argument, it will cross-tabulate the arguments, as the following code demonstrates.
           > table(Dec=schools$Dec, Auth=schools$Auth)
               Auth
           Dec  Other Private State State Integrated
110259 12 200230 22 302208 35 406219 28 502214 38 602215 34 706188 45 8 0 11200 45 9 0 12205 37 10038205 31
Again, the result of this function call is a table data structure. This time it is a two-dimensional table, which is very similar to a two-dimensional array.
In this example, we have provided names for the arguments, Dec and Auth, and these have been used to provide overall labels for each dimension of the table.

This table has one row for each decile and one column for each type of ownership. For example, the third row of the table tells us that there are 2 private schools, 209 state schools, and 35 state integrated schools with a decile of 3.
The most obvious feature of this result is that private schools tend to be in wealthier areas, with higher deciles.
Another function that can be used to generate tables of counts is the xtabs() function. This is very similar to the table() function, except that the factors to cross-tabulate are specified in a formula, rather than as separate arguments.
The two-dimensional table above could also be generated with the following code.
           > xtabs( ~ Dec + Auth, data=schools)
One advantage of this approach is that the symbols used in the formula are automatically found in the data frame provided in the data argument, so, for example, there is no need to specify the Auth variable as schools$Auth, as we had to do in the previous call to the table() function.
9.8.5 Aggregation
R provides many functions for calculating numeric summaries of data. For example, the min() and max() functions calculate minimum and maximum values for a vector, sum() calculates the sum, and the mean() function calculates the average value. The following code calculates the average value of the number of students enrolled at New Zealand schools.
           > mean(schools$Roll)
[1] 295.4737
This “grand mean” value is not very interesting by itself. What would be more interesting would be to calculate the average enrollment for different types of schools and compare, for example, state schools and private schools.
We could calculate these values individually, by extracting the relevant subset of the data. For example, the following code extracts just the enrollments for private schools and calculates the mean for those values.
           > mean(schools$Roll[schools$Auth == "Private"])
[1] 308.798
However, a better way to work is to let R determine the subsets and calculate all of the means in one go. The aggregate() function provides one way to perform this sort of task.
The following code uses aggregate() to calculate the average enrollment for New Zealand schools broken down by school ownership.
There are four different values of schools$Auth, so the result is four averages. We can easily check the answer for Other schools because there is only one such school and we saw on page 321 that this school has a roll of 51.
           > aggregate(schools["Roll"],
                        by=list(Ownership=schools$Auth),
FUN=mean)
                    Ownership     Roll
                        Other  51.0000
                      Private 308.7980
                        State 300.6301
           4 State Integrated 258.3792
This result shows that the average school size is remarkably similar for all types of school ownership (ignoring the "Other" case because there is only one such school).
The aggregate() function can take a bit of getting used to, so we will take a closer look at the arguments to this function.
There are three arguments to the aggregate() call. The first argument provides the values that we want to average. In this case, these values are the enrolments for each school. A minor detail is that we have provided a data frame with only one column, the Roll variable, rather than just a vector. It is possible to provide either a vector or a data frame, but the advantage of using a data frame is that the second column in the result has a sensible name.
We could have used schools$Roll, a vector, instead of schools["Roll"], a data frame, but then the right-hand column of the result would have had
1 2 3

an uninformative name, x.
The second argument to the aggregate() function is called by and the value must be a list. In this case, there is only one component in the list, the Auth variable, which describes the ownership of each school. This argument is used to generate subsets of the first argument. In this case, we subset the school enrollments into four groups, corresponding to the four different types of school ownership. By providing a name for the list component, Ownership, we control the name of the first column in the result. If we had not done this, then the first column of the result would have had a much less informative name, Group.1.
The third argument to the aggregate() function, called FUN, is the name of a function. This function is called on each subset of the data. In this case, we have specified the mean() function, so we have asked for the average enrollments to be calculated for each different type of school ownership. It is possible to specify any function name here, as long as the function returns only a single value as its result. For example, sum(), min(), and max() are all possible alternatives.
The diagram below provides a conceptual view of how aggregate() works.
In the terminology of data queries in Section 7.2.1, the aggregate() function acts very much like an SQL query with a GROUP BY clause.

Another point to make about the aggregate() function is that the value returned by the function is a data frame. This is convenient because data frames are easy to understand and we have lots of tools for working with data frames.
We will now look at a slightly more complicated use of the aggregate() function. For this example, we will generate a new vector called rich that records whether each school’s decile is greater than 5. The following code creates the new vector.
           > rich <- schools$Dec > 5
> rich
           [1] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE ...
The vector rich provides a crude measure of whether each school is in a wealthy area or not. We will now use this variable in a more complicated call to aggregate().
Because the by argument to aggregate() is a list, it is possible to provide more than one factor. This means that we can produce a result for all possible combinations of two or more factors. In the following code, we provide a list of two factors as the by argument, so the result will be the average enrolments broken down by both the type of school ownership and whether the school is in a wealthy area.
           > aggregate(schools["Roll"],
                        by=list(Ownership=schools$Auth,
                                Rich=rich),
                        FUN=mean)
         Ownership  Rich     Roll
             Other FALSE  51.0000
           Private FALSE 151.4000
             State FALSE 261.7487
4 State Integrated FALSE 183.2370
5          Private  TRUE 402.5362
6            State  TRUE 338.8243
7 State Integrated  TRUE 311.2135
1 2 3

The result of the aggregation is again a data frame, but this time there are three columns. The first two columns indicate the different groups and the third gives the average for each group.
The result suggests that, on average, schools in wealthier areas have more students.
One limitation of the aggregate() function is that it only works with functions that return a single value. If we want to calculate, for example, the range of enrollments for each type of school—the minimum and the maximum together—the range() function will perform the calculation for us, but we cannot use range() with aggregate() because it returns two values. Instead, we need to use the by() function.
The following code calculates the range of enrollments for all New Zealand schools, demonstrating that the result is a numeric vector containing two values, 5 and 5546.
           > range(schools$Roll)
           [1]    5 5546
The following code uses the by() function to generate the range of enrollments broken down by school ownership.
           > rollRanges <-
                  by(schools["Roll"],
                     INDICES=list(Ownership=schools$Auth),
                     FUN=range)
           > rollRanges
           Ownership: Other
           [1] 51 51
           ---------------------------------------------
           Ownership: Private
           [1]    7 1663
           ---------------------------------------------
           Ownership: State
           [1]    5 5546
           ---------------------------------------------
           Ownership: State Integrated
           [1]   18 1475
The arguments to the by() function are very similar to the arguments for aggregate() (with some variation in the argument names) and the effect is also very similar: the first argument is broken into subsets, with one subset for each different value in the second argument, and the third argument provides a function to call on each subset.
However, the result of by() is not a data frame like for aggregate(). It is a very different sort of data structure.
           > class(rollRanges)
           [1] "by"
We have not seen this sort of data structure before, but a by object behaves very much like a list, so we can work with this result just like a list.
The result of the call to by() gives the range of enrollments for each type of school ownership. It suggests that most of the large schools are owned by the state.
9.8.6 Case study: NCEA
In order to motivate some of the remaining sections, here we introduce another, related New Zealand schools data set for us to work with.
The National Certificates of Educational Achievement (NCEA) are used to measure students’ learning in New Zealand secondary schools. Students usually attempt to achieve NCEA Level 1 in their third year of secondary schooling, Level 2 in their fourth year, and Level 3 in their fifth and final year of secondary school.
Each year, information on the percentage of students who achieved each NCEA level is reported for all New Zealand secondary schools. In this case study, we will look at NCEA achievement percentages for 2007.
The data are stored in a plain text, colon-delimited format, in a file called NCEA2007.txt. There are four columns of data: the school name, plus the three achievement percentages for the three NCEA levels. Figure 9.10 shows the first few lines of the file.
The following code uses the read.table() function to read these data into a data frame called NCEA. We have to specify the colon delimiter via sep=":". Also, because some school names have an apostrophe, we need to specify quote="". Otherwise, the apostrophes are interpreted as the start of a text field. The last two arguments specify that the first line of the file contains

            Name:Level1:Level2:Level3
            Al-Madinah School:61.5:75:0
            Alfriston College:53.9:44.1:0
            Ambury Park Centre for Riding Therapy:33.3:20:0
            Aorere College:39.5:50.2:30.6
Auckland Girls' Grammar School:71.2:78.9:55.5 Auckland Grammar:22.1:30.8:26.3
Auckland Seventh-Day Adventist H S:50.8:34.8:48.9 Avondale College:57.3:49.8:44.6
            Baradene College:89.3:89.7:88.6
            ...
Figure 9.10: The first few lines of the file NCEA2007.txt. This file contains information about the percentage of students gaining NCEA qualifications at New Zealand secondary schools in 2007. This is a plain text, colon-delimited format.
the variable names (header=TRUE) and that the first column of school names should be treated as character data (not as a factor).
           > NCEA <- read.table(file.path("Schools", "NCEA2007.txt"),
                                 sep=":", quote="",
> NCEA
header=TRUE, as.is=TRUE)
             Name Level1 Level2 Level3
1                     Al-Madinah School
2                     Alfriston College
3 Ambury Park Centre for Riding Therapy
61.5   75.0    0.0
53.9   44.1    0.0
33.3   20.0    0.0
39.5   50.2   30.6
71.2   78.9   55.5
22.1   30.8   26.3
50.8   34.8   48.9
4
5
6
7 ...
Aorere College Auckland Girls' Grammar School Auckland Grammar Auckland Seventh-Day Adventist H S
As with the schools data frame, the data set is too large to view all at once, so we will only show a few rows for most examples in the following sections.
The dim() function tells us that, in total, there are 88 schools and 4 variables in this data set. This data set only has Auckland schools.

> dim(NCEA)
[1]88 4
We will now explore this data set in order to demonstrate some more data manipulation techniques.
9.8.7 The “apply” functions
The NCEA data frame contains three columns of numeric data—the percentage of students achieving NCEA at the three different NCEA levels. Something we could quickly look at is whether the achievement percentages are similar, on average, for each NCEA level.
One way to look at these averages is to extract each column and calculate an average for each column. For example, the following code calculates the average achievement percentage for NCEA Level 1.
           > mean(NCEA$Level1)
[1] 62.26705
However, as in the aggregation case, there is a smarter way to work, which is to let R extract the columns and calculate a mean for all columns in a single function call. This sort of task is performed by the apply() function.
The following code uses the apply() function to calculate the average achievement percentage for each NCEA level.
           > apply(NCEA[2:4], MARGIN=2, FUN=mean)
             Level1   Level2   Level3
           62.26705 61.06818 47.97614
The result suggests that a slightly lower percentage of students achieve NCEA Level 3 compared to the other NCEA levels.
As with the aggregate() function, the apply() function can take a bit of getting used to, so we will now take a closer look at how the apply() function works.
The apply() function takes three main arguments.
The first argument is expected to be a matrix (or array). In the example above, we have provided the second, third, and fourth columns of the NCEA data frame, i.e., a data frame with three columns, as the first argument. This is an example of a situation that was mentioned back in Section 9.6.2, where a function will silently coerce the value that we supply to another sort of data structure if it needs to.
In this case, the apply() function silently coerces the data frame that we give it into a matrix. The conversion from a data frame to a matrix makes sense in this case because all three columns of the data frame that we supplied are numeric, so the data frame columns convert very naturally and predictably into a matrix (with three columns and 88 rows).
The second argument to apply() specifies how the array should be broken into subsets. The value 1 means split the matrix into separate rows and the value 2 means split the matrix into separate columns. In this case, we have split the matrix into columns; each column represents percentages for one NCEA level.
The third argument specifies a function to call for each subset. In this case, the apply() call says to take each column corresponding to an NCEA level and call the mean() function on each column.
The diagram below provides a conceptual view of how apply() works when MARGIN=1 (apply by rows). Figure 9.11 includes a diagram that illustrates using apply() by columns.
The data structure returned by apply() depends on how many values are returned by the function FUN. In the last example, we used the function mean(), which returns just a single value, so the overall result was a numeric vector, but if FUN returns more than one value, the result will be a matrix.

For example, the following code calls apply() with the range() function to get the range of percentages at each NCEA level. The range() function returns two values for each NCEA level, so the result is a matrix with three columns (one for each NCEA level) and two rows—the minimum and maximum percentage at each level.
           > apply(NCEA[2:4], 2, range)
                Level1 Level2 Level3
           [1,]    2.8    0.0    0.0
           [2,]   97.4   95.7   95.7
The basic idea of apply() has similarities with the aggregate() and by() functions that we saw in the previous section. In all of these cases, we are breaking a larger data structure into smaller pieces, calling a function on each of the smaller pieces and then putting the results back together again to create a new data structure.
In the case of aggregate(), we start with a vector or a data frame and we end up with a data frame. The by() function starts with a vector or a data frame and ends up with a list-like object. With apply(), we start with an array and end up with a vector or a matrix.
The lapply() function is another example of this idea. In this case, the starting point is a list. The function breaks the list into its separate components, calls a function on each component, and then combines the results into a new list (see Figure 9.11).
In order to demonstrate the use of lapply(), we will repeat the task we just performed using apply().
The following code calls lapply() to calculate the average percentage of students achieving each NCEA level.
           > lapply(NCEA[2:4], FUN=mean)
           $Level1
           [1] 62.26705
           $Level2
           [1] 61.06818
           $Level3
           [1] 47.97614

The lapply() function has only two main arguments. The first argument is a list. As with the apply() example, we have supplied a data frame— the second, third, and fourth columns of the NCEA data frame—rather than supplying a list. The lapply() function silently converts the data frame to a list, which is a natural conversion if we treat each variable within the data frame as a component of the list.
The second argument to lapply() is a function to call on each component of the list, and again we have supplied the mean() function.
The result of lapply() is a list data structure. The numeric results, the average percentages for each NCEA level, are exactly the same as they were when we used apply(); all that has changed is that these values are stored in a different sort of data structure.
Like apply(), the lapply() function comfortably handles functions that return more than a single value. For example, the following code calculates the range of percentages for each NCEA level.
           > lapply(NCEA[2:4], FUN=range)
           $Level1
           [1]  2.8 97.4
           $Level2
           [1]  0.0 95.7
           $Level3
           [1]  0.0 95.7
These results from lapply() are very similar to the previous results from apply(); all that has changed is that the values are stored in a different sort of data structure (a list rather than a matrix).
Whether to use apply() or lapply() will depend on what sort of data structure we are starting with and what we want to do with the result. For example, if we start with a list data structure, then lapply() is the appropriate function to use. However, if we start with a data frame, we can often use either apply() or lapply() and the choice will depend on whether we want the answer as a list or as a matrix.
The sapply() function provides a slight variation on lapply(). This function behaves just like lapply() except that it attempts to simplify the result by returning a simpler data structure than a list, if that is possible. For example, if we use sapply() rather than lapply() for the previous examples, we get a vector and a matrix as the result, rather than lists.
           > sapply(NCEA[2:4], mean)
             Level1   Level2   Level3
           62.26705 61.06818 47.97614
           > sapply(NCEA[2:4], range)
                Level1 Level2 Level3
           [1,]    2.8    0.0    0.0
           [2,]   97.4   95.7   95.7
9.8.8 Merging
In this section, we will look at the problem of combining data structures. For simple situations, the functions c(), cbind(), and rbind() are all that
we need. For example, consider the two numeric vectors below.
> 1:3
[1] 1 2 3
> 4:6
[1] 4 5 6
The c() function is useful for combining vectors or lists to make a longer vector or a longer list.
> c(1:3, 4:6)
[1] 1 2 3 4 5 6
The functions cbind() and rbind() can be used to combine vectors, matrices, or data frames with similar dimensions. The cbind() function creates a matrix or data frame by combining data structures side-by-side (a “column” bind) and rbind() combines data structures one above the other (a “row” bind).
           > cbind(1:3, 4:6)
                [,1] [,2]
           [1,]    1    4
           [2,]    2    5
           [3,]    3    6
           > rbind(1:3, 4:6)
                [,1] [,2] [,3]
           [1,]    1    2    3
           [2,]    4    5    6
A more difficult problem arises if we want to combine two data structures that do not have the same dimensions.
In this situation, we need to first find out which rows in the two data structures correspond to each other and then combine them.
The diagram below illustrates how two data frames (left and right) can be merged to create a single data frame (middle) by matching up rows based on a common column (labeled m in the diagram).

For example, the two case studies that we have been following in this section provide two data frames that contain information about New Zealand schools. The schools data frame contains information about the location and size of every primary and secondary school in the country, while the NCEA data frame contains information about the NCEA performance of every secondary school.
It would be useful to be able to combine the information from these two data frames. This would allow us to, for example, compare NCEA performance between schools of different sizes.
The merge() function can be used to combine data sets like this.
The following code creates a new data frame, aucklandSchools, that contains information from the schools data frame and from the NCEA data frame for Auckland secondary schools.
                 > aucklandSchools <- merge(schools, NCEA,
                                             by.x="Name", by.y="Name",
                                             all.x=FALSE, all.y=FALSE)
                 > aucklandSchools

                               Name ID      City    Auth Dec Roll Level1 Level2 Level3
35  Mahurangi College 24 Warkworth
48      Orewa College 25     Orewa
32   Long Bay College 27  Auckland
55  Rangitoto College 28  Auckland   State  10 3022   85.0   81.7   71.6
30     Kristin School 29  Auckland Private  10 1624   93.4   27.8   36.7
19  Glenfield College 30  Auckland   State   7  972   58.4   65.5   45.6
10 Birkenhead College 31  Auckland   State   6  896   59.8   65.7   50.4
...
The first two arguments to the merge() function are the data frames to be merged, in this case, schools and NCEA.
The by.x and by.y arguments specify the names of the variables to use to match the rows from one data frame with the other. In this case, we use the Name variable from each data frame to determine which rows belong with each other.
The all.x argument says that the result should only include schools from the schools data frame that have the same name as a school in the NCEA data frame. The all.y argument means that a school in NCEA must have a match in schools or it is not included in the final result.
By default, merge() will match on any variable names that the two data frames have in common, and it will only include rows in the final result if there is a match between the data frames. In this case, we could have just used the following expression to get the same result:
> merge(schools, NCEA)
9.8.9 Flashback: Database joins
In the terminology of Section 7.2.4, the merge() function is analogous to a database join.
In relational database terms, by.x and by.y correspond to a primary- key/foreign-key pairing. The all.x and all.y arguments control whether the join is an inner join or an outer join.
The merge() call above roughly corresponds to the following SQL query.
SELECT *
    FROM schools INNER JOIN NCEA
        ON schools.Name = NCEA.Name;
State   8 1095
State   9 1696
State  10 1611
71.9   66.2   55.9
75.2   81.0   54.9
74.5   84.2   67.2

9.8.10 Splitting
Instead of combining two data frames, it is sometimes useful to break a data frame into several pieces. For example, we might want to perform a separate analysis on the New Zealand schools for each different type of school ownership.
The split() function can be used to break up a data frame into smaller data frames, based on the levels of a factor. The result is a list, with one component for each level of the factor.
The diagram below illustrates how the split() function uses the values within a factor to break a data frame (left) into a list containing two separate data frames (right).

The result is effectively several subsets of the original data frame, but with the subsets all conveniently contained within a single overall data structure (a list), ready for subsequent processing or analysis.
In the following code, the Rolls variable from the schools data frame is broken into four pieces based on the Auth variable.
           > schoolRollsByAuth <- split(schools$Roll, schools$Auth)
The resulting list, schoolRollsByAuth, has four components, each named after one level of the Auth variable and each containing the number of students enrolled at schools of the appropriate type. The following code uses the str() function that we saw in Section 9.6.7 to show a brief view of the basic structure of this list.
           > str(schoolRollsByAuth)

List of 4
 $ Other
 $ Private
 $ State
: int 51
: int [1:99] 255 39 154 73 83 25 95 85 ..
: int [1:2144] 318 200 455 86 577 329 6..
 $ State Integrated: int [1:327] 438 26 191 560 151 114 12..
Because the result of split() is a list, it is often used in conjunction with lapply() to perform a task on each component of the resulting list. For example, we can calculate the average roll for each type of school ownership with the following code.
> lapply(schoolRollsByAuth, mean)
$Other [1] 51
$Private
[1] 308.798
$State
[1] 300.6301
$`State Integrated` [1] 258.3792
This result can be compared to the result we got using the aggregate() function on page 323.
9.8.11 Reshaping
For multivariate data sets, where a single individual or case is measured several times, there are two common formats for storing the data.
The so-called “wide” format uses a separate variable for each measurement, and the “long” format for a data set has a single observation on each row.
The diagram below illustrates the difference between wide format (on the left), where each row contains multiple measurements (t1, t2, and t3) for a single individual, and long format (on the right), where each row contains a single measurement and there are multiple rows for each individual.

For some data processing tasks, it is useful to have the data in wide format, but for other tasks, the long format is more convenient, so it is useful to be able to convert a data set from one format to the other.
The NCEA data frame that we have been using is in wide format because it has a row for each school and a column for each NCEA level. There is one row per school and three observations on each row.
> NCEA
1                     Al-Madinah School
2                     Alfriston College
3 Ambury Park Centre for Riding Therapy
61.5   75.0
53.9   44.1
33.3   20.0
39.5   50.2
71.2   78.9
22.1   30.8
50.8   34.8
0.0 0.0 0.0
   30.6
   55.5
   26.3
   48.9
three rows, below. We
4
5
6
7 ...
Aorere College Auckland Girls' Grammar School Auckland Grammar Auckland Seventh-Day Adventist H S
In the long format of the NCEA data set, each school would have one for each NCEA level. What this format looks like is shown

Name Level1 Level2 Level3

will go on to show how to obtain this format.
                                              Name variable value
           1                     Alfriston College   Level1  53.9
           2                     Alfriston College   Level2  44.1
           3                     Alfriston College   Level3   0.0
           4                     Al-Madinah School   Level1  61.5
           5                     Al-Madinah School   Level2  75.0
           6                     Al-Madinah School   Level3   0.0
           7 Ambury Park Centre for Riding Therapy   Level1  33.3
...
There are several ways to perform the transformation between wide and long formats in R, but we will focus in this section on the reshape package because it provides a wider range of options and a more intuitive set of arguments.
           > library("reshape")
The two main functions in the reshape package are called melt() and cast(). The melt() function is used to convert a data frame into long format, and the cast() function can then be used to reshape the data into a variety of other forms.
Melting
The following code creates the long format version of the NCEA data frame using the melt() function.
           > longNCEA <- melt(NCEA, measure.var=2:4)
The melt() function typically requires two arguments. The first argument provides the data frame to melt. The second argument is either measure.var to specify which columns of the data frame are observations (reshape calls them “measured” variables) or id.var to specify which columns of the data frame are just values that identify the individual or case (reshape calls these “id” variables).
In the example above, we have specified that the second, third, and fourth columns are observations. The melt() function infers that the first column is therefore an “id” variable. The following code produces an identical result, using the id.var argument instead. In this case, we have specified that the Name variable is an “id” variable and the melt() function will treat all other columns as “measured” variables. This code also demonstrates that columns in the data frame can be specified by name as well as by number.
           > longNCEA <- melt(NCEA, id.var="Name")
                                              Name variable value
           1                     Alfriston College   Level1  53.9
           2                     Alfriston College   Level2  44.1
           3                     Alfriston College   Level3   0.0
           4                     Al-Madinah School   Level1  61.5
           5                     Al-Madinah School   Level2  75.0
           6                     Al-Madinah School   Level3   0.0
           7 Ambury Park Centre for Riding Therapy   Level1  33.3
...
The longNCEA data frame has 264 rows because each of the original 88
schools now occupies three rows.
           > dim(longNCEA)
[1] 264 3
The original four columns in NCEA have become three columns in longNCEA. The first column in longNCEA is the same as the first column of NCEA, except that all of the values repeat three times.
The second column in longNCEA is called variable and it records which column of NCEA that each row of longNCEA has come from. For example, all values from the original Level1 column in NCEA have the value Level1 in the new variable column in longNCEA.
The third column in longNCEA is called value and this contains all of the data from the Level1, Level2, and Level3 columns of NCEA.
Casting
We now turn our attention to the other main function in the reshape
package, the cast() function.

As a simple demonstration of the cast() function, we will use it to recreate the original wide format of the NCEA data set. The following code performs this transformation.
> cast(longNCEA, Name ~ variable)
1                     Alfriston College
2                     Al-Madinah School
3 Ambury Park Centre for Riding Therapy
53.9   44.1    0.0
61.5   75.0    0.0
33.3   20.0    0.0
39.5   50.2   30.6
71.2   78.9   55.5
22.1   30.8   26.3
50.8   34.8   48.9
4
5
6
7 ...
The
The
Aorere College Auckland Girls' Grammar School Auckland Grammar Auckland Seventh-Day Adventist H S
first argument to cast() is a data frame in long format. second argument to cast() is a formula.
Variables on the left-hand side of this formula are used to form the rows of
the result and variables on the right-hand side are used to form columns. In the above example, the result contains a row for each different school, based on the Name of the school, and a column for each different NCEA level, where the variable column specifies the NCEA level.
The following code uses a different formula to demonstrate the power that cast() provides for reshaping data. In this case, the data set has been transposed so that each column is now a different school and each row corresponds to one of the NCEA levels.
> tNCEA <- cast(longNCEA, variable ~ Name)
> tNCEA
1 2 3
Level1              53.9
Level2              44.1
Level3               0.0
61.5 ...
75.0 ...
 0.0 ...
variable Alfriston College Al-Madinah School ...

Name Level1 Level2 Level3

start   therms  gas KWHs
elect   temp    days
                   10-Jun-98       9
                   20-Jul-98       6
                   18-Aug-98       7
                   16-Sep-98       42
                   19-Oct-98       105
                   17-Nov-98       106
                   17-Dec-98       200
                   19-Jan-99       144
                   18-Feb-99       179
...
Figure 9.12: space-delimited format.
This data structure has 3 rows and 89 columns: one row for each NCEA level and one column for each school, plus an extra column containing the names of the NCEA levels.
> dim(tNCEA)
[1] 389
Now that we have seen a number of techniques for manipulating data sets, the next section presents a larger case study that combines several of these techniques to perform a more complex exploration of a data set.
9.8.12 Case study: Utilities
A resident of Baltimore, Maryland, in the United States collected data from his residential gas and electricity power bills over 8 years. The data are in a text file called baltimore.txt and include the start date for the bill, the number of therms of gas used and the amount charged, the number of kilowatt hours of electricity used and the amount charged, the average daily outdoor temperature (as reported on the bill), and the number of days in the billing period. Figure 9.12 shows the first few lines of the data file.
Several events of interest occurred in the household over the time period during which these values were recorded, and one question of interest was to determine whether any of these events had any effect on the energy consumption of the household.
The events were:
ii ii
16.84   613
15.29   721
15.73   597
35.81   460
77.28   314
77.01   342
136.66  298
107.28  278
122.80  253
63.80   75      40
74.21   76      29
62.22   76      29
43.98   70      33
31.45   57      29
33.86   48      30
30.08   40      33
28.37   37      30
26.21   39      29
The first few lines of the Utilities data set in a plain text, white-

An additional resident moved in on July 31st 1999.
Two storm windows were replaced on April 22nd 2004.
Four storm windows were replaced on September 1st 2004. 
An additional resident moved in on December 18th 2005.
The text file containing the energy usage can be read conveniently using the read.table() function, with the header=TRUE argument specified to use the variable names on the first line of the file. We also use as.is=TRUE to keep the dates as text for now.
           > utilities <- read.table(file.path("Utilities",
                                                "baltimore.txt"),
                                      header=TRUE, as.is=TRUE)

> utilities
                 start therms   gas KWHs elect temp days
1 10-Jun-98
2 20-Jul-98
3 18-Aug-98
4 16-Sep-98
5 19-Oct-98
6 17-Nov-98
...
  9 16.84  613 63.80
  6 15.29  721 74.21
  7 15.73  597 62.22
 42 35.81  460 43.98
105 77.28  314 31.45
106 77.01  342 33.86
75   40
76   29
76   29
70   33
57   29
48   30
The first thing we want to do is to convert the first variable into actual
dates. This will allow us to perform calculations and date values.
> utilities$start <- as.Date(utilities$start,
                              "%d-%b-%y")
> utilities
       start therms   gas KWHs elect temp days
comparisons on the
1 1998-06-10
2 1998-07-20
3 1998-08-18
4 1998-09-16
5 1998-10-19
6 1998-11-17
...
  9 16.84  613 63.80
  6 15.29  721 74.21
  7 15.73  597 62.22
 42 35.81  460 43.98
105 77.28  314 31.45
106 77.01  342 33.86
75   40
76   29
76   29
70   33
57   29
48   30
The next thing that we will do is break the data set into five different time “phases,” using the four significant events as breakpoints; we will be interested in the average daily charges for each of these phases.
To keep things simple, we will just determine the phase based on whether the billing period began before a significant event.
The six critical dates that we will use to categorize each billing period are the start of the first billing period, the dates at which the significant events occurred, and the start of the last billing period.

           > breakpoints <- c(min(utilities$start),
                               as.Date(c("1999-07-31", "2004-04-22",
                                         "2004-09-01", "2005-12-18")),
                               max(utilities$start))
           > breakpoints
           [1] "1998-06-10" "1999-07-31" "2004-04-22" "2004-09-01"
           [5] "2005-12-18" "2006-08-17"
We can use the cut() function to convert the start variable, which contains the billing period start dates, into a phase variable, which is a factor.
           > phase <- cut(utilities$start, breakpoints,
                           include.lowest=TRUE, labels=FALSE)
> phase
[1] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [29] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [57] 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 4 4 4 4 4 5 4 4 4 4 4 4 [85] 4 4 4 4 4 5 5 5 5 5 5 5
Each billing period now has a corresponding phase.
One important point to notice about these phase values is that they are not strictly increasing. In particular, the 78th value is a 5 amongst a run of 4s. This reveals that the billing periods in the original file were not entered in strictly chronological order.
Now that we have each billing period assigned to a corresponding phase, we can sum the energy usage and costs for each phase. This is an application of the aggregate() function. The following code subsets the utilities data frame to remove the first column, the billing period start dates, and then sums the values in all of the remaining columns for each different phase.
           > phaseSums <- aggregate(utilities[c("therms", "gas", "KWHs",
                                                 "elect", "days")],
                                     list(phase=phase), sum)

           > phaseSums
             phase therms     gas  KWHs   elect days
1 1
2 2
3 3
4     4   1737 2350.63  5872  567.91  483
5     5    577  847.38  3608  448.61  245
 875  703.27  5883  602.84  406
5682 5364.23 24173 2269.14 1671
  28   76.89  1605  170.27  124
We can now divide the totals for each phase by the number of days in each phase to obtain an average usage and cost per day for each phase.
> phaseAvgs <- phaseSums[2:5]/phaseSums$days
For inspecting these values, it helps if we round the values to two significant figures.
> signif(phaseAvgs, 2)
  therms  gas KWHs elect
1 2.20 1.70
2 3.40 3.20
3 0.23 0.62
4 3.60 4.90
5 2.40 3.50
14   1.5
14   1.4
13   1.4
12   1.2
15   1.8
The division step above works because when a data frame is divided by a vector, each variable in the data frame gets divided by the vector. This is not necessarily obvious from the code; a more explicit way to perform the operation is to use the sweep() function, which forces us to explicitly state that, for each row of the data frame (MARGIN=1), we are dividing (FUN="/") by the corresponding value from a vector (STAT=phase$days).
> phaseSweep <- sweep(phaseSums[2:5],
                       MARGIN=1, STAT=phaseSums$days, FUN="/")
Looking at the average daily energy values for each phase, the values that stand out are the gas usage and cost during phase 3 (after the first two storm windows were replaced, but before the second set of four storm windows were replaced). The naive interpretation is that the first two storm windows were incredibly effective, but the next four storm windows actually made things worse again!

At first sight this appears strange, but it is easily explained by the fact that phase 3 coincided with summer months, as shown below using the table() function. This code produces a rough count of how many times each month occurred in each phase. The function months() extracts the names of the months from the dates in the start variable; each billing period is assigned to a particular month based on the month that the period started in (which is why this is a rough count).
           > table(months(utilities$start), phase)[month.name, ]
phase 12345
January 15011 February 15011 March 05011 April 15011 May 13111 June 24111 July 24111 August 15111 September 1 5 0 2 0
October
November
December
15020 14020 15020
Subsetting the resulting table by the predefined symbol month.name just rearranges the order of the rows of the table; this ensures that the months are reported in calendar order rather than alphabetical order.
We would expect the gas usage (for heating) to be a lot lower during summer. This is confirmed by (roughly) calculating the daily average usage and cost for each month.
Again we must take care to get the answer in calendar order; this time we do that by creating a factor that is based on converting the start of each billing period into a month, with the order of the levels of this factor explicitly set to be month.name.
> billingMonths <- factor(months(utilities$start),
                           levels=month.name)
We then use aggregate() to sum the usage, costs, and days for the billing periods, grouped by this billingMonths factor.

           > months <-
                  aggregate(utilities[c("therms", "gas", "KWHs",
                                        "elect", "days")],
                            list(month=billingMonths),
sum)
Finally, we divide the usage and cost totals by the number of days in the billing periods. So that we can easily comprehend the results, we cbind() the month names on to the averages and round the averages using signif().
           > cbind(month=months$month,
                    signif(months[c("therms", "gas",
                                    "KWHs", "elect")]/months$days,
                           2))
                  month therms  gas KWHs elect
1    January
2   February
3      March
4      April
5        May
6       June
7       July
8     August
9  September   1.20 1.30   14   1.3
10   October   3.40 3.50   12   1.1
11  November   5.40 5.50   14   1.2
12  December   7.20 7.10   13   1.2
These results are presented graphically in Figure 9.13.
This example has demonstrated a number of data manipulation techniques in a more realistic setting. The simple averages that we have calculated serve to show that any attempt to determine whether the significant events led to a significant change in energy consumption and cost for this household is clearly going to require careful analysis.
7.80 7.50
5.90 5.90
3.90 4.00
1.70 1.90
0.65 0.99
0.22 0.57
0.21 0.59
0.23 0.60
13   1.2
11   1.0
12   1.1
11   1.0
12   1.3
16   1.8
22   2.3
17   1.8

Figure 9.13: A plot of the average daily energy usage, by month, for the Utilities data set. Gas usage is represented by the solid line and electricity usage is represented by the dashed line.

Recap
Simple manipulations of data include sorting values, binning numeric variables, and producing tables of counts for categorical variables.
Aggregation involves producing a summary value for each group in a data set.
An “apply” operation calculates a summary value for each variable in a data frame, or for each row or column of a matrix, or for each component of a list.
Data frames can be merged just like tables can be joined in a database. They can also be split apart.
Reshaping involves transforming a data set from a row-per-subject, “wide”, format to a row-per-observation, “long”, format (among other things).

9.9 Text processing
The previous section dealt with techniques that are useful for for manipulating a variety of data structures and a variety of data types.
This section is focused solely on data manipulation techniques for working with vectors containing character values.
Plain text is a very common format for storing information, so it is very useful to be able to manipulate text. It may be necessary to convert a data set from one text format to another. It is also common to search for and extract important keywords or specific patterns of characters from within a large set of text.
We will introduce some of the basic text processing ideas via a simple example.
9.9.1 Case study: The longest placename
One of the longest placenames in the world is attributed to a hill in the Hawke’s Bay region of New Zealand. The name (in Maori) is ...
Taumatawhakatangihangakoauauotamateaturipukakapikimaungahoronukupokaiwhenuakitanatahu
... which means “The hilltop where Tamatea with big knees, conqueror of mountains, eater of land, traveler over land and sea, played his koauau [flute] to his beloved.”
Children at an Auckland primary school were given a homework assignment that included counting the number of letters in this name. This task of counting the number of characters in a piece of text is a simple example of what we will call text processing and is the sort of task that often comes up when working with data that have been stored in a text format.
Counting the number of characters in a piece of text is something that any programming language will do. Assuming that the name has been saved into a text file called placename.txt, here is how to use the scan() function to read the name into R, as a character vector of length 1.
                 > placename <- scan(file.path("Placename", "placename.txt"),
                                      "character")
The first argument provides the name and location of the file and the second argument specifies what sort of data type is in the file. In this case, we are reading a single character value from a file.
We can now use the nchar() function to count the number of characters in this text.
           > nchar(placename)
[1] 85
Counting characters is a very simple text processing task, though even with something that simple, performing the task using a computer is much more likely to get the right answer. We will now look at some more complex text processing tasks.
The homework assignment went on to say that, in Maori, the combinations ‘ng’ and ‘wh’ can be treated as a single letter. Given this, how many letters are in the placename? There are two possible approaches: convert every ‘ng’ and ‘wh’ to a single letter and recount the number of letters, or count the number of ‘ng’s and ‘wh’s and subtract that from the total number of characters. We will consider both approaches because they illustrate two different text processing tasks.
For the first approach, we could try counting all of the ‘ng’s and ‘wh’s as single letters by searching through the text and converting all of them into single characters and then redoing the count. In R, we can perform this search-and-replace task using the gsub() function, which takes three arguments: a pattern to search for, a replacement value, and the text to search within. The result is the original text with the pattern replaced. Because we are only counting letters, it does not matter which letter we choose as a replacement. First, we replace occurrences of ‘ng’ with an underscore character.
           > replacengs <- gsub("ng", "_", placename)
           > replacengs
[1] "Taumatawhakata_iha_akoauauotamateaturipukakapikimau_ahoronukupokaiwhenuakitanatahu"
Next, we replace the occurrences of ‘wh’ with an underscore character. > replacewhs <- gsub("wh", "_", replacengs)
> replacewhs
[1] "Taumata_akata_iha_akoauauotamateaturipukakapikimau_ahoronukupokai_enuakitanatahu"
Finally, we count the number of letters in the resulting text.
           > nchar(replacewhs)
[1] 80
The alternative approach involves just finding out how many ‘ng’s and ‘wh’s are in the text and subtracting that number from the original count. This simple step of searching within text for a pattern is yet another common text processing task. There are several R functions that perform variations on this task, but for this example we need the function gregexpr() because it returns all of the matches within a piece of text. This function takes two arguments: a pattern to search for and the text to search within. The result is a vector of the starting positions of the pattern within the text, with an attribute that gives the lengths of each match. Figure 9.14 includes a diagram that illustrates how this function works.
           > ngmatches <- gregexpr("ng", placename)[[1]]
           > ngmatches
           [1] 15 20 54
           attr(,"match.length")
           [1] 2 2 2
           attr(,"useBytes")
           [1] TRUE
This result shows that the pattern ‘ng’ occurs three times in the placename, starting at character positions 15, 20, and 54, respectively, and that the length of the match is 2 characters in each case. Here is the result of searching for occurrences of ‘wh’:
           > whmatches <- gregexpr("wh", placename)[[1]]
           > whmatches
[1] 870 attr(,"match.length") [1] 2 2 attr(,"useBytes")
[1] TRUE

The return value of gregexpr() is a list to allow for more than one piece of text to be searched at once. In this case, we are only searching a single piece of text, so we just need the first component of the result.
We can use the length() function to count how many matches there were in the text.
           > length(ngmatches)
           [1] 3
> length(whmatches)
[1] 2
The final answer is simple arithmetic.
           > nchar(placename) -
                (length(ngmatches) + length(whmatches))
[1] 80
For the final question in the homework assignment, the students had to count how many times each letter appeared in the placename (treating ‘wh’ and ‘ng’ as separate letters again).
One way to do this in R is by breaking the placename into individual characters and creating a table of counts. Once again, we have a standard text processing task: breaking a single piece of text into multiple pieces. The strsplit() function performs this task in R. It takes two arguments: the text to break up and a pattern which is used to decide where to split the text. If we give the value NULL as the second argument, the text is split at each character. Figure 9.14 includes a diagram that illustrates how this function works.

           > nameLetters <- strsplit(placename, NULL)[[1]]
           > nameLetters
            [1] "T" "a" "u" "m" "a" "t" "a" "w" "h" "a" "k" "a" "t" "a"
           [15] "n" "g" "i" "h" "a" "n" "g" "a" "k" "o" "a" "u" "a" "u"
           [29] "o" "t" "a" "m" "a" "t" "e" "a" "t" "u" "r" "i" "p" "u"
           [43] "k" "a" "k" "a" "p" "i" "k" "i" "m" "a" "u" "n" "g" "a"
           [57] "h" "o" "r" "o" "n" "u" "k" "u" "p" "o" "k" "a" "i" "w"
           [71] "h" "e" "n" "u" "a" "k" "i" "t" "a" "n" "a" "t" "a" "h"
           [85] "u"
Again, the result is a list to allow for breaking up multiple pieces of text at once. In this case, because we only have one piece of text, we are only interested in the first component of the list.
One minor complication is that we want the uppercase ‘T’ to be counted as a lowercase ‘t’. The function tolower() performs this task.
           > lowerNameLetters <- tolower(nameLetters)
           > lowerNameLetters
            [1] "t" "a" "u" "m" "a" "t" "a" "w" "h" "a" "k" "a" "t" "a"
           [15] "n" "g" "i" "h" "a" "n" "g" "a" "k" "o" "a" "u" "a" "u"
           [29] "o" "t" "a" "m" "a" "t" "e" "a" "t" "u" "r" "i" "p" "u"
           [43] "k" "a" "k" "a" "p" "i" "k" "i" "m" "a" "u" "n" "g" "a"
           [57] "h" "o" "r" "o" "n" "u" "k" "u" "p" "o" "k" "a" "i" "w"
           [71] "h" "e" "n" "u" "a" "k" "i" "t" "a" "n" "a" "t" "a" "h"
           [85] "u"
Now it is a simple matter of calling the tablefunction to produce a table of counts of the letters.
           > letterCounts <- table(lowerNameLetters)
           > letterCounts
lowerNameLetters aeghikmnoprtuw
22 2 3 5 6 8 3 6 5 3 2 8 10 2
As well as pulling text apart into smaller pieces as we have done so far, we also need to be able to put several pieces of text together to make a single larger piece of text.
For example, if we begin with the individual letters of the placename, as in the character vector nameLetters, how do we combine the letters to make a single character value? In R, this can done with the paste() function.
The paste() function can be used to combine separate character vectors or to combine the character values within a single character vector. In this case, we want to perform the latter task.
We have a character vector containing 85 separate character values.
           > nameLetters
            [1] "T" "a" "u" "m" "a" "t" "a" "w" "h" "a" "k" "a" "t" "a"
           [15] "n" "g" "i" "h" "a" "n" "g" "a" "k" "o" "a" "u" "a" "u"
           [29] "o" "t" "a" "m" "a" "t" "e" "a" "t" "u" "r" "i" "p" "u"
           [43] "k" "a" "k" "a" "p" "i" "k" "i" "m" "a" "u" "n" "g" "a"
           [57] "h" "o" "r" "o" "n" "u" "k" "u" "p" "o" "k" "a" "i" "w"
           [71] "h" "e" "n" "u" "a" "k" "i" "t" "a" "n" "a" "t" "a" "h"
           [85] "u"
The following code combines the individual character values to make the complete placename. The collapse argument specifies that the character vector should be collapsed into a single character value with, in this case (collapse=""), nothing in between each character.
           > paste(nameLetters, collapse="")
[1] "Taumatawhakatangihangakoauauotamateaturipukakapikimaungahoronukupokaiwhenuakitanatahu"
This section has introduced a number of functions for counting letters in text, transforming text, breaking text apart, and putting it back together again. More examples of the use of these functions are given in the next section and in case studies later on.
9.9.2 Regular expressions
Two of the tasks we looked at when working with the long Maori placename in the previous case study involved treating both ‘ng’ and ‘wh’ as if they were single letters by replacing them both with underscores. We performed the task in two steps: convert all occurrences of ‘ng’ to an underscore, and then convert all occurrences of ‘wh’ to an underscore. Conceptually, it would be simpler, and more efficient, to perform the task in a single step: convert all occurrences of ‘ng’ and ‘wh’ to underscore characters. Regular expressions allow us to do this.
With the placename in the variable called placename, converting both ‘ng’ and ‘wh’ to underscores in a single step is achieved as follows:
           > gsub("ng|wh", "_", placename)
[1] "Taumata_akata_iha_akoauauotamateaturipukakapikimau_ahoronukupokai_enuakitanatahu"
The regular expression we are using, ng|wh, describes a pattern: the character ‘n’ followed by the character ‘g’ or the character ‘w’ followed by the character ‘h’. The vertical bar, |, is a metacharacter. It does not have its normal meaning, but instead denotes an optional pattern; a match will occur if the text contains either the pattern to the left of the vertical bar or the pattern to the right of the vertical bar. The characters ‘n’, ‘g’, ‘w’, and ‘h’ are all literals; they have their normal meaning.
A regular expression consists of a mixture of literal characters, which have their normal meaning, and metacharacters, which have a special meaning. The combination describes a pattern that can be used to find matches amongst text values.
A regular expression may be as simple as a literal word, such as cat, but regular expressions can also be quite complex and express sophisticated ideas, such as [a-z]{3,4}[0-9]{3}, which describes a pattern consisting of either three or four lowercase letters followed by any three digits.
Just like all of the other technologies in this book, there are several different versions of regular expressions, however, rather than being numbered, the different versions of regular expressions have different names: there are Basic regular expressions, Extended (POSIX) regular expressions, and Perl-Compatible regular expressions (PCRE). We will assume Extended regular expressions in this book, though for the level of detail that we encounter, the differences between Extended regular expressions and PCRE are not very important. Basic regular expressions, as the name suggests, have fewer features than the other versions.
There are two important components required to use regular expressions: we need to be able to write regular expressions and we need software that understands regular expressions in order to search text for the pattern specified by the regular expression.
We will focus on the use of regular expressions with R in this book, but there are many other software packages that understand regular expressions, so it should be possible to use the information in this chapter to write effective regular expressions within other software environments as well. One caveat is that R consumes backslashes in text, so it is necessary to type a double backslash in order to obtain a single backslash in a regular expression. This means that we often avoid the use of backslashes in regular expressions in R and, when we are forced to use backslashes, the regular expressions will look more complicated than they would in another software setting.
Something to keep in mind when writing regular expressions (especially when trying to determine why a regular expression is not working) is that most software that understands regular expressions will perform “eager” and “greedy” searches. This means that the searches begin from the start of a piece of text, the first possible match succeeds (even if a “better” match might exist later in the text), and as many characters as possible are matched. In other words, the first and longest possible match is found by each component of a regular expression. A common problem is to have a later component of a regular expression fail to match because an earlier component of the expression has consumed the entire piece of text all by itself.
For this reason, it is important to remember that regular expressions are a small computer language of their own and should be developed with just as much discipline and care as we apply to writing any computer code. In particular, a complex regular expression should be built up in smaller pieces in order to understand how each component of the regular expression works before adding further components.
The next case study looks at some more complex uses and provides some more examples. Chapter 11 describes several other important metacharacters that can be used to build more complex regular expressions.
9.9.3 Case study: Rusty wheat
As part of a series of field trials conducted by the Institut du V ́eg ́etal in France,3 data were gathered on the effect of the disease Septoria tritici on wheat. The amount of disease on individual plants was recorded using data collection forms that were filled in by hand by researchers in the field.
In 2007, due to unusual climatic conditions, two other diseases, Puccinia recondita (“brown rust”) and Puccinia striiformis (“yellow rust”) were also observed to be quite prevalent. The data collection forms had no specific field for recording the amount of rust on each wheat plant, so data were
3Thanks to David Gouache, Arvalis—Institut du V ́eg ́etal.

recorded ad hoc in a general area for “diverse observations”.
In this case study, we will not be interested in the data on Septoria tritici. Those data were entered into a standard form using established protocols, so the data were relatively tidy.
Instead, we will focus on the yellow rust and brown rust data because these data were not anticipated, so the data were recorded quite messily. The data included other comments unrelated to rust and there were variations in how the rust data were expressed by different researchers.
This lack of structure means that the rust data cannot be read into R using the functions that expect a regular format, such as read.table() and read.fwf() (see Section 9.7.3). This provides us with an example where text processing tools allow us to work with data that have an irregular structure.
The yellow and brown rust data were transcribed verbatim into a plain text file, as shown in Figure 9.15.
Fortunately, for the purposes of recovering these results, some basic features of the data are consistent.
Each line of data represents one wheat plant. If brown rust was present, the line contains the letters rb, followed by a space, followed by a number indicating the percentage of the plant affected by the rust (possibly with a percentage sign). If the plant was afflicted by yellow rust, the same pattern applies except that the letters rj are used. It is possible for both diseases to be present on the same plant (see the last line of data in Figure 9.15).
The abbreviations rb and rj were used because the French common names for the diseases are rouille brune and rouille jaune.
For this small set of recordings, the data could be extracted by hand. However, the full data set contains many more records so we will develop a code solution that uses regular expressions to recover the rust data from these recordings.
The first step is to get the data into R. We can do this using the readLines() function, which will create a character vector with one element for each line of recordings.

lema, rb 2%
rb 2%
rb 3%
rb 4%
rb 3%
rb 2%,mineuse
rb
rb
rb 12
rb
rj 30%
rb
rb
rb 25%
rb
rb
rb
rj 10, rb 4
Figure 9.15:
wheat plants, in a largely unstructured plain text format. Each line represents one wheat plant. The text rb followed by a number represents an amount of brown rust and the text rj followed by a number represents an amount of yellow rust.
Data recording the occurrence of brown or yellow rust diseases on

           > wheat <- readLines(file.path("Wheat", "wheat.txt"))
           > wheat
 [1] "lema, rb 2%"
 [4] "rb 4%"
 [7] "rb"
[10] "rb"
[13] "rb"
[16] "rb"
"rb 2%"
"rb 3%"
"rb"
"rj 30%"
"rb 25%"
"rb"
"rb 3%"
"rb 2%,mineuse"
"rb 12"
"rb"
"rb"
"rj 10, rb 4"
What we want to end up with are two variables, one recording the amount of brown rust on each plant and one recording the amount of yellow rust.
Starting with brown rust, the first thing we could do is find out which plants have any brown rust on them. The following code does this using the grep() function. The result is a vector of indices that tells us which lines contain the pattern we are searching for. Figure 9.14 includes a diagram that illustrates how this function works.
> rbLines <- grep("rb [0-9]+", wheat)
> rbLines
[1]  1  2  3  4  5  6  9 14 18
The regular expression in this call demonstrates two more important examples of meta-characters. The square brackets, ‘[’ and ‘]’, are used to describe a character set that will be matched. Within the brackets we can specify individual characters or, as in this case, ranges of characters; 0-9 means any character between ‘0’ and ‘9’.
The plus sign, ‘+’, is also a meta-character, known as a modifier. It says that whatever immediately precedes the plus sign in the regular expression can repeat several times. In this case, [0-9]+ will match one or more digits.
The letters ‘r’, ‘b’, and the space are all literal, so the entire regular expression will match the letters rb, followed by a space, followed by one or more digits. Importantly, this pattern will match anywhere within a line; the line does not have to begin with rb. In other words, this will match rows on which brown rust has been observed on the wheat plant.
Having found which lines contain information about brown rust, we want to extract the information from those lines. The indices from the call to grep() can be used to subset out just the relevant lines of data.

> wheat[rbLines]
[1] "lema, rb 2%"
[4] "rb 4%"
[7] "rb 12"
"rb 2%"
"rb 3%"
"rb 25%"
"rb 3%"
"rb 2%,mineuse"
"rj 10, rb 4"
We will extract just the brown rust information from these lines in two steps, partly so that we can explore more about regular expressions, and partly because we have to in order to cater for plants that have been afflicted by both brown and yellow rust.
The first step is to reduce the line down to just the information about brown rust. In other words, we want to discard everything except the pattern that we are looking for, rb followed by a space, followed by one or more digits. The following code performs this step.
> rbOnly <- gsub("^.*(rb [0-9]+).*$", "\\1",
                  wheat[rbLines])
> rbOnly
[1]"rb2" "rb2" "rb3" "rb4" "rb3" "rb2" "rb12" [8] "rb 25" "rb 4"
The overall strategy being used here is to match the entire line of text, from the first character to the last, but within that line we want to identify and isolate the important component of the line, the brown rust part, so that we can retain just that part.
Again, we have some new meta-characters to explain. First is the “hat” character, ‘^’, which matches the start of the line (or the start of the text). Next is the full stop, ‘.’. This will match any single character, no matter what it is. The ‘*’ character is similar to the ‘+’; it modifies the immediately preceding part of the expression and allows for zero or more occurrences. An expression like ^.* allows for any number of characters at the start of the text (including zero characters, or an empty piece of text).
The parentheses, ‘(’ and ‘)’, are used to create sub-patterns within a regular expression. In this case, we are isolating the pattern rb [0-9]+, which matches the brown rust information that we are looking for. Parentheses are useful if we want a modifier, like ‘+’ or ‘*’, to affect a whole sub-pattern rather than a single character, and they can be useful when specifying the replacement text in a search-and-replace operation, as we will see below.
After the parenthesized sub-pattern, we have another .* expression to allow for any number of additional characters and then, finally, a dollar sign, ‘$’.

The latter is the counterpart to ‘^’; it matches the end of a piece of text.
Thus, the complete regular expression explicitly matches an entire piece of text that contains information on brown rust. Why do we want to do this? Because we are going to replace the entire text with only the piece that we want to keep. That is the purpose of the back-reference, "\\1", in the replacement text.
The text used to replace a matched pattern in gsub() is mostly just literal text. The one exception is that we can refer to sub-patterns within the regular expression that was used to find a match. By specifying "\\1", we are saying reuse whatever matched the sub-pattern within the first set of parentheses in the regular expression. This is known as a back-reference. In this case, this refers to the brown rust information.
The overall meaning of the gsub() call is therefore to replace the entire text with just the part of the text that contains the information about brown rust.
Now that we have character values that contain only the brown rust information, the final step we have to perform is to extract just the numeric data from the brown rust information. We will do this in three ways in order to demonstrate several different techniques.
One approach is to take the text values that contain just the brown rust information and throw away everything except the numbers. The following code does this using a regular expression.
           > gsub("[^0-9]", "", rbOnly)
           [1] "2"  "2"  "3"  "4"  "3"  "2"  "12" "25" "4"
The point about this regular expression is that it uses ^ as the first character within the square brackets. This has the effect of negating the set of characters within the brackets, so [^0-9] means any character that is not a digit. The effect of the complete gsub() call is to replace anything that is not a digit with an empty piece of text, so only the digits remain.
An alternative approach is to recognize that the text values that we are dealing with have a very regular structure. In fact, all we need to do is drop the first three characters from each piece of text. The following code does this with a simple call to substring().

           > substring(rbOnly, 4)
           [1] "2"  "2"  "3"  "4"  "3"  "2"  "12" "25" "4"
The first argument to substring() is the text to reduce and the second argument specifies which character to start from. In this case, the first character we want is character 4. There is an optional third argument that specifies which character to stop at, but if, as in this example, the third argument is not specified, then we keep going to the end of the text.
The name of this function comes from the fact that text values, or character values, are also referred to as strings.
The final approach that we will consider works with the entire original text, wheat[rbLines], and uses a regular expression containing an extra set of parentheses to isolate just the numeric content of the brown rust information as a sub-pattern of its own. The replacement text refers to this second sub-pattern, "\\2", so it reduces the entire line to only the part of the line that is the numbers within the brown rust information, in a single step.
           > gsub("^.*(rb ([0-9]+)).*$", "\\2", wheat[rbLines])
           [1] "2"  "2"  "3"  "4"  "3"  "2"  "12" "25" "4"
We are not quite finished because we want to produce a variable that contains the brown rust information for all plants. We will use NA for plants that were not afflicted.
A simple way to do this is to create a vector of NAs and then fill in the rows for which we have brown rust information. The other important detail in the following code is the conversion of the textual information into numeric values using as.numeric().
           > rb <- rep(NA, length(wheat))
           > rb[rbLines] <- as.numeric(gsub("^.*(rb ([0-9]+)).*$",
                                             "\\2", wheat[rbLines]))
> rb
[1] 2 2 3 4 3 2NANA12NANANANA25NANANA 4
To complete the exercise, we need to repeat the process for yellow rust. Rather than repeat the approach used for brown rust, we will investigate a different solution, which will again allow us to demonstrate more text processing techniques.
This time, we will use regexpr() rather than grep() to find the lines that we want. We are now searching for the lines containing yellow rust data. > rjData <- regexpr("rj [0-9]+", wheat)
> rjData
[1]-1-1-1-1-1-1-1-1-1-1 1-1-1-1-1-1-1 1 attr(,"match.length")
[1]-1-1-1-1-1-1-1-1-1-1 5-1-1-1-1-1-1 5 attr(,"useBytes")
[1] TRUE
The result is a numeric vector with a positive number for lines that contain yellow rust data and -1 otherwise. The number indicates the character where the data start. Figure 9.14 includes a diagram that illustrates how this function works.
In this case, there are only two lines containing yellow rust data (lines 11 and 18) and, in both cases, the data start at the first character.
The result also has an attribute called match.length, which contains the number of characters that produced the match with the regular expression that we were searching for. In both cases, the pattern matched a total of 5 characters: the letters r and j, followed by a space, followed by two digits. This length information is particularly useful because it will allow us to extract the yellow rust data immediately using substring(). This time we specify both a start and an end character for the subset of the text.
           > rjText <- substring(wheat, rjData,
                                  attr(rjData, "match.length"))
> rjText
[1]"" "" "" "" "" "" ""
            [8] ""      ""      ""      "rj 30" ""      ""      ""
           [15] ""      ""      ""      "rj 10"
Obtaining the actual numeric data can be carried out using any of the techniques we described above for the brown rust case.
The following code produces the final result, including both brown and yellow rust as a data frame.

           > rj <- as.numeric(substring(rjText, 4))
           > data.frame(rb=rb, rj=rj)
rb rj 1 2NA 2 2NA 3 3NA 4 4NA 5 3NA 6 2NA
7 NANA
8 NANA
9 12NA
10 NA NA 11 NA 30 12 NA NA 13 NA NA 14 25 NA 15 NA NA 16 NA NA 17 NA NA 18 410
Recap
Text processing includes: searching within text for a pattern; replacing the text that matches a pattern; splitting text into smaller pieces; combining smaller pieces of text into larger pieces of text; and converting other types of data into text.

9.10 Data display
As we have seen in most of the sections so far in this chapter, most of the tasks that we perform with data, using a programming language, work with the data in RAM. We access data structures that have been stored in RAM and we create new data structures in RAM.
Something that we have largely ignored to this point is how we get to see the data on the computer screen.
The purpose of this section is to address the topic of formatting data values for display.
The important thing to keep in mind is that we do not typically see raw computer memory on screen; what we see is a display of the data in a format that is fit for human consumption.
There are two main ways that information is displayed on a computer screen: as text output or as graphical output (pictures or images). R has sophisticated facilities for graphical output, but that is beyond the scope of this book and we will not discuss those facilities here. Instead, we will focus on displaying text output to the screen.
For a start, we will look a little more closely at how R automatically displays values on the screen.

9.10.1 Case study: Point Nemo (continued)
In this section we will work with a subset of the temperature values from the Point Nemo data set (see Section 1.1).
The temperature values have previously been read into R as the temp variable in the data frame called pointnemodelim (see Section 9.7.4). In this section, we will only work with the first 12 of these temperature values, which represent the first year’s worth of monthly temperature recordings.
                 > twelveTemps <- pointnemodelim$temp[1:12]
                 > twelveTemps
                  [1] 278.9 280.0 278.9 278.9 277.8 276.1 276.1 275.6 275.6
                 [10] 277.3 276.7 278.9
The data structure that we are dealing with is a numeric vector. The values in twelveTemps are stored in RAM as numbers.
However, the display that we see on screen is text. The values are numeric, so the characters in the text are mostly digits, but it is important to realize that everything that R displays on screen for us to read is a text version of the data values.
The function that displays text versions of data values on screen is called print(). This function gets called automatically to display the result of an R expression, but we can also call it directly, as shown below. The display is exactly the same as when we type the name of the symbol by itself.
                 > print(twelveTemps)
                  [1] 278.9 280.0 278.9 278.9 277.8 276.1 276.1 275.6 275.6
                 [10] 277.3 276.7 278.9
One reason for calling print() directly is that this function has arguments that control how values are displayed on screen. For example, when displaying numeric values, there is an argument digits that controls how many significant digits are displayed.
In the following code, we use the digits argument to only display three digits for each temperature value. This has no effect on the values in RAM; it only affects how the numbers are converted to text for display on the screen.

           > print(twelveTemps, digits=3)
            [1] 279 280 279 279 278 276 276 276 276 277 277 279
The print() function is a generic function (see Section 9.6.8), so what gets displayed on screen is very different for different sorts of data structures; the arguments that provide control over the details of the display will also vary.
Although print() has some arguments that control how information is displayed, it is not completely flexible. For example, when printing out a numeric vector, as above, it will always print out the index, in this case [1], at the start of the line.
If we want to have complete control over what gets displayed on the screen, we need to perform the task in two steps: first, generate the text that we want to display, and then call the cat() function to display the text on the screen.
For simple cases, the cat() function will automatically coerce values to a character vector. For example, the following code uses cat() to display the twelveTemps numeric vector on screen. The fill argument says that a new line should be started after 60 characters have been used.
           > cat(twelveTemps, fill=60)
           278.9 280 278.9 278.9 277.8 276.1 276.1 275.6 275.6 277.3
           276.7 278.9
The difference between this display and what print() displays is that there is no index at the start of each row. This is the usefulness of cat(): it just displays values and does not perform any formatting of its own. This means that we can control the formatting when we generate text values and then just use cat() to get the text values displayed on screen.
In summary, the problem of producing a particular display on screen is essentially a problem of generating a character vector in the format that we require and then calling cat().
The next section looks at the problem of generating character vectors in a particular format.

9.10.2 Converting to text
We have previously seen two ways to convert data values to character values: some functions, e.g., as.character(), perform an explicit type coercion from an original data structure to a character vector; and some functions, e.g., paste(), automatically coerce their arguments to character vectors. In this section, we will look at some more functions that perform explicit coercion to character values.
The following code coerces the twelveTemps numeric vector to a character vector using as.character().
                 > as.character(twelveTemps)
                  [1] "278.9" "280"   "278.9" "278.9" "277.8" "276.1" "276.1"
                  [8] "275.6" "275.6" "277.3" "276.7" "278.9"
One thing to notice about this result is that the second value, "280", is only three characters long, whereas all of the other values are five characters long.
This is a small example of a larger problem that arises when converting values, particularly numbers, to text; there are often many possible ways to perform the conversion. In the case of converting a real number to text, one major problem is how many decimal places to use.
There are several functions in R that we can use to resolve this problem.
The format() function produces character values that have a “common format.” What that means depends on what sorts of values are being formatted, but in the case of a numeric vector, it means that the resulting character values are all of the same length. In the following result, the second value is five characters long, just like all of the other values.
                 > format(twelveTemps)
                  [1] "278.9" "280.0" "278.9" "278.9" "277.8" "276.1" "276.1"
                  [8] "275.6" "275.6" "277.3" "276.7" "278.9"
The format() function has several arguments that provide some flexibility in the result, but its main benefit is that it displays all values with a common appearance.
For complete control over the conversion to text values, there is the sprintf() function.

The following code provides an example of the use of sprintf() that converts the twelveTemps numeric vector into a character vector where every numeric value is converted to a character value with two decimal places and a total of nine characters, followed by a space and a capital letter ‘K’ (for degrees Kelvin).
> sprintf(fmt="%9.2f K", twelveTemps)
[1] "
[5] "
[9] "
278.90 K" "
277.80 K" "
275.60 K" "
280.00 K" "
276.10 K" "
277.30 K" "
278.90 K" "
276.10 K" "
276.70 K" "
278.90 K"
275.60 K"
278.90 K"
The first argument to sprintf(), called fmt, defines the formatting of the values. The value of this argument can include special codes, like %9.2f. The first special code within the fmt argument is used to format the second argument to sprintf(), in this case the numeric vector twelveTemps.
There are a number of special codes for controlling the formatting of different types of values; the components of the format in this example are shown below.
start of special code: real number format: nine characters in total: two decimal places: literal text:
%9.2f K
%9.2f K
%9.2f K
%9.2f K
%9.2f K
With the twelveTemps formatted like this, we can now use cat() to display the values on the screen in a format that is quite different from the display produced by print().
> twelveTemps
 [1] 278.9 280.0 278.9 278.9 277.8 276.1 276.1 275.6 275.6
[10] 277.3 276.7 278.9
> cat(sprintf("%9.2f K", twelveTemps), fill=60)
278.90 K
276.10 K
276.70 K
280.00 K    278.90 K    278.90 K    277.80 K
276.10 K    275.60 K    275.60 K    277.30 K
278.90 K

This sort of formatting can also be useful if we need to generate a plain text file with a particular format. Having generated a character vector as above, this can easily be written to an external text file using the writeLines() function or by specifying a filename for cat() to write to. If the file argument is specified in a call to cat(), then output is written to an external file rather than being displayed on the screen.
9.10.3 Results for reports
One reason for using a particular format for displaying results is so that the results can be conveniently included in a research report.
For example, way back in Section 2.1, we saw a very basic web page report about the Pacific and Eurasian Poles of Inaccessibility. The web page is reproduced in Figure 9.16.
This web page includes a summary table showing the range of temperature values for both Poles of Inaccessibility. R code that generates the table of ranges is shown below.
           > pointnemotemps <-
                  read.fwf(file.path("LAS", "pointnemotemp.txt"),
                           skip=8, widths=c(-23, 5),
                           col.names=c("temp"))
           > eurasiantemps <-
                  read.fwf(file.path("LAS", "eurasiantemp.txt"),
                           skip=8, widths=c(-23, 5),
                           col.names=c("temp"))
           > allTemps <- cbind(pacific=pointnemotemps$temp,
                                eurasian=eurasiantemps$temp)
           > ranges <- round(rbind(min=apply(allTemps, 2, min),
> ranges
    pacific eurasian
min     276      252
max     283      307
max=apply(allTemps, 2, max)))
The HTML code that includes this table in the web page is reproduced below. This is the most basic way to make use of R output; simply copy the R display directly into another document, with a monospace font.
<pre>

Figure 9.16: A simple web page that displays information about the surface i temperature data for the Pacific and Eurasian Poles of Inaccessibility (viewed
with the Firefox web browser on Windows XP). This is a reproduction of Figure 2.1.

               pacific eurasian
           min     276      252
           max     283      307
</pre>
However, this approach produces a very plain display. A more sophisticated approach is to format the R result using the same technology as is used to produce the report. For example, in the case of a web page report, we could create an HTML table to display the result.
Several R packages provide functions to carry out this task. For example, the hwriter package has an hwrite() function that converts an R table into text describing an HTML table.
           > library(hwriter)
           > cat(hwrite(ranges))
           <table border="1">
           <tr>
           <td></td><td>pacific</td><td>eurasian</td></tr>
           <tr>
           <td>min</td><td>276</td><td>252</td></tr>
           <tr>
           <td>max</td><td>283</td><td>307</td></tr>
           </table>
This approach allows us to integrate R results within a report more naturally and more aesthetically.
It is worth noting that this is just a text processing task; we are converting the values from the R table into text values and then combining those text values with HTML tags, which are just further text values.
This is another important advantage of carrying out tasks by writing computer code; we can use a programming language to write computer code. We can write code to generate our instructions to the computer, which is a tremendous advantage if the instructions are repetitive, for example, if we write the same HTML report every month.
Another option for producing HTML code from R data structures is the xtable package; this package can also format R data structures as LATEX tables.
Although a detailed description is beyond the scope of this book, it is also worth mentioning the Sweave package, which allows HTML (or LATEX) code to be combined with R code within a single document, thereby avoiding having to cut-and-paste results from R to a separate report document by hand.
Recap
When we view data values that have been stored in RAM, what we see displayed on screen is typically a text version of the data values.
The conversion of data values to a text representation is sometimes ambiguous and requires us to provide a specification of what the result should be.
It is possible to format the display of R data structures so that they can be integrated nicely within research reports.
9.11 Programming
A computer program is often described as “data structures + algorithms.”
Most of this chapter has been concerned with data structures—how information is stored in computer memory—and a number of tools that allow us to convert one data structure into another sort of data structure in a single step.
Algorithms are the series of steps that we take in order to carry out more complex tasks. Algorithms are what we use to combine smaller tasks into computer programs.
The difference between data structures and algorithms is like the difference between the ingredients that we need to cook a meal and the recipe that we use to cook the meal. The focus of this book is on data structures. Much of what we need to achieve when processing a data set can be performed in only a few steps, with only one or two expressions. This book is mostly about boiling an egg rather than baking a souffl ́e.
However, as we perform more complex data processing tasks, it becomes useful to know something about how to combine and manage a larger number of expressions.
In this section, we will look at some slightly more advanced ideas for writing computer code. This treatment will barely scratch the surface of the topics available; the aim is to provide a very brief introduction to some useful ideas

                         VARIABLE : Mean Near-surface air temperature (kelvin)
                         FILENAME : ISCCPMonthly_avg.nc
                         FILEPATH : /usr/local/fer_data/data/
                         SUBSET   : 24 by 24 points (LONGITUDE-LATITUDE)
                         TIME     : 16-JAN-1995 00:00
                          113.8W 111.2W 108.8W 106.2W 103.8W 101.2W 98.8W  ...
                           27     28     29     30     31     32     33    ...
             36.2N / 51:  272.1  270.3  270.3  270.9  271.5  275.6  278.4  ...
             33.8N / 50:  282.2  282.2  272.7  272.7  271.5  280.0  281.6  ...
             31.2N / 49:  285.2  285.2  276.1  275.0  278.9  281.6  283.7  ...
             28.8N / 48:  290.7  286.8  286.8  276.7  277.3  283.2  287.3  ...
             26.2N / 47:  292.7  293.6  284.2  284.2  279.5  281.1  289.3  ...
             23.8N / 46:  293.6  295.0  295.5  282.7  282.7  281.6  285.2  ...
             ...
Figure 9.17: The first few lines of output from the Live Access Server for the near-surface air temperature of the earth for January 1995, over a coarse 24 by 24 grid of locations covering Central America.
for writing larger amounts of more complex programming code.
We will use a case study to motivate the need for these more advanced techniques.
9.11.1 Case study: The Data Expo (continued)
The data for the 2006 JSM Data Expo (Section 5.2.8) were obtained from NASA’s Live Access Server as a set of 505 text files.
Seventy-two of those files contain near-surface air temperature measurements, with one file for each month of recordings. Each file contains average temperatures for the relevant month at 576 different locations. Figure 9.17 shows the first few lines of the temperature file for the first month, January 1995.
With the data expo files stored in a local directory, NASA/Files, the complete set of 72 filenames for the files containing temperature recordings can be generated by the following code (only the first six filenames are shown).

           > nasaAirTempFiles <-
                  file.path("NASA", "Files",
                            paste("temperature", 1:72, ".txt",
                                  sep=""))
           > head(nasaAirTempFiles)
           [1] "NASA/Files/temperature1.txt"
           [2] "NASA/Files/temperature2.txt"
           [3] "NASA/Files/temperature3.txt"
           [4] "NASA/Files/temperature4.txt"
           [5] "NASA/Files/temperature5.txt"
           [6] "NASA/Files/temperature6.txt"
We will conduct a simple task with these data: calculating the near-surface air temperature for each month, averaged over all locations. In other words, we will calculate a single average temperature from each file. The result will be a vector of 72 monthly averages.
As a starting point, the following code reads in the data from just a single file, using the first filename, temperature1.txt, and averages all of the temperatures in that file.
           > tempDF <- read.fwf(nasaAirTempFiles[1],
                             skip=7,
                             widths=c(-12, rep(7, 24)))
           > tempMatrix <- as.matrix(tempDF)
           > mean(tempMatrix)
[1] 295.1849
The call to read.fwf() ignores the first 7 lines of the file and the first 12 characters on each of the remaining lines of the file. This just leaves the temperature values, which are stored in RAM as a data frame, tempDF. This data frame is then converted into a matrix, tempMatrix, so that, in the final step, the mean() function can calculate the average across all of the temperature values.
At this point, we have simple code that uses the functions and data structures that have been described in previous sections. The next step is to repeat this task for all 72 air temperature files.

One way to do that is to write out 72 almost identical copies of the same piece of code, but, unsurprisingly, there are ways to work much more efficiently. The next section describes how.
9.11.2 Control flow
In a programming language, if our code contains more than one expression, the expressions are run one at a time, in the order that they appear.
For example, consider the following two expressions from the code in the previous section.
           tempMatrix <- as.matrix(tempDF)
           mean(tempMatrix)
In this code, the expression mean(tempMatrix) relies on the fact that the previous expression has already been run and that there is a matrix data structure stored in RAM for it to access via the symbol tempMatrix.
However, programming languages also provide ways to modify this basic rule and take control of the order in which expressions are run. One example of this is the idea of a loop, which is a way to allow a collection of expressions to be run repeatedly.
Returning to our example of calculating an average value for each of 72 different files, the following code shows how to calculate these averages using a loop in R.
           > avgTemp <- numeric(72)
           > for (i in 1:72) {
                  tempDF <- read.fwf(nasaAirTempFiles[i],
                                 skip=7,
                                 widths=c(-12,
                                   rep(7, 24)))
                  tempMatrix <- as.matrix(tempDF)
                  avgTemp[i] <- mean(tempMatrix)
              }
The first expression is just a setup step that creates a vector of 72 zeroes. This vector will be used to store the 72 average values as we calculate them.

The components of the actual loop are shown below.
keywords: parentheses: loop symbol: loop symbol values: open bracket: loop body:
close bracket:
for (i in 1:72) {
for (i in 1:72) {
for (i in 1:72) {
for (i in 1:72) {
for (i in 1:72) {
    tempDF <- read.fwf(nasaAirTempFiles[i],
                   skip=7,
                   widths=c(-12,
                     rep(7, 24)))
    tempMatrix <- as.matrix(tempDF)
    avgTemp[i] <- mean(tempMatrix)
}
The keywords, for and in, the brackets, and the parentheses will be the same in any loop that we write. The most important bits are the loop symbol, in this case the symbol i, and the values that the loop symbol can take, in this case, 1:72.
The idea of a loop is that the expressions in the body of the loop are run several times. Specifically, the expressions in the body of the loop are run once for each value of the loop symbol. Also, each time the loop repeats, a different value is assigned to the loop symbol.
In this example, the value 1 is assigned to the symbol i and the expressions in the body of the loop are run. Then, the value 2 is assigned to the symbol i and the body of the loop is run again. This continues until we assign the value 72 to the symbol i and run the body of the loop, and then the loop ends.
If we look closely at the expressions in the body of the loop, we can see that the loop symbol, i, is used twice. In the first case, nasaAirTempFiles[i], this means that each time through the loop we will read a different air temperature file. In the second case, avgTemp[i], this means that the average that we calculate from the air temperature file will be stored in a different element of the vector avgTemp, so that by the end of the loop, we have all 72 averages stored together in the vector.
The important idea is that, although the code in the body of the loop remains constant, the code in the body of the loop can do something slightly different each time the loop runs because the loop symbol changes its value every time the loop runs.
The overall result is that we read all 72 files, calculate an average from each one, and store the 72 averages in the vector avgTemp, which is shown below. 

> avgTemp
            [1] 295.1849 295.3175 296.3335 296.9587 297.7286 298.5809
            [7] 299.1863 299.0660 298.4634 298.0564 296.7019 295.9568
           [13] 295.3915 296.1486 296.1087 297.1007 298.3694 298.1970
           [19] 298.4031 298.0682 298.3148 297.3823 296.1304 295.5917
           [25] 295.5562 295.6438 296.8922 297.0823 298.4793 299.3575
           [31] 299.7984 299.7314 299.6090 298.4970 297.9872 296.8453
           [37] 296.9569 296.9354 297.0240 298.0668 299.1821 300.7290
           [43] 300.6998 300.3715 300.1036 299.2269 297.8642 297.2729
           [49] 296.8823 297.4288 297.5762 298.2859 299.1076 299.1938
           [55] 299.0599 299.5424 298.9135 298.2849 297.0981 296.2639
           [61] 296.1943 296.5868 297.5510 298.6106 299.7425 299.5219
           [67] 299.7422 300.3411 299.5781 298.6965 297.0830 296.3813
9.11.3 Writing functions
When we have to write code to perform the same task several times, one approach, as described in the previous section, is to write a loop.
In this section, we will look at another option: writing functions.
In our example, we want to calculate an overall average for each of 72 separate files. One way to look at this task is that there are 72 filenames stored in a character vector data structure and we want to perform a calculation for each element of this data structure. As we saw in Section 9.8.7, there are several R functions that are designed to perform a task on each element of a data structure.
What we can do in this case is use sapply() to call a function once for each of the filenames in the vector nasaAirTempFiles.
The call to the sapply() function that we would like to make is shown below.
           sapply(nasaAirTempFiles, calcMonthAvg)
The bad news is that the function calcMonthAvg() does not exist! There is no predefined function that can read a NASA air temperature file and return the average of all of the values in the file.
The good news is that we can very easily create this function ourselves.

Here is code that creates a function called calcMonthAvg() to perform this task.
           > calcMonthAvg <- function(filename) {
                  tempDF <- read.fwf(filename,
skip=7,
                                     widths=c(-12, rep(7, 24)))
                  tempMatrix <- as.matrix(tempDF)
                  mean(tempMatrix)
              }
This code creates a function and assigns it to the symbol calcMonthAvg. In most examples in this chapter, we have stored data structures in RAM, but in this case, we have stored a function in RAM; we have stored some computer code in RAM. Just like with any other symbol, we can retrieve the function from RAM by referring to the symbol calcMonthAvg.
           > calcMonthAvg
           function (filename)
           {
               tempDF <- read.fwf(filename, skip = 7,
                   widths = c(-12, rep(7, 24)))
               tempMatrix <- as.matrix(tempDF)
               mean(tempMatrix)
           }
However, because this is a function, we can do more than just retrieve its value; we can also call the function. This is done just like any other function call we have ever made. We append parentheses to the symbol name and include arguments within the parentheses. The following code calls our new function to calculate the average for the first air temperature file.
           > calcMonthAvg(nasaAirTempFiles[1])
[1] 295.1849
In order to understand what happens when we call our function, we need to take a closer look at the components of the code that was used to create the function.

function name: keyword: parentheses: argument symbol: open bracket: function body:
close bracket:

 calcMonthAvg <- function(filename) {
 calcMonthAvg <- function(filename) {
 calcMonthAvg <- function(filename) {
 calcMonthAvg <- function(filename) {
 calcMonthAvg <- function(filename) {
     tempDF <- read.fwf(filename,
                        skip=7,
                        widths=c(-12,
                                 rep(7, 24)))
     tempMatrix <- as.matrix(tempDF)
     mean(tempMatrix)
}
The keyword function, the brackets, and the parentheses will be the same for every function that we write. The important bits are the name we choose for the function and the argument symbols that we choose for the function.
The idea of a function is that the expressions that make up the body of the function will be run when the function is called. Also, when the function is called, the value that is provided as the argument in the function call will be assigned to the argument symbol.
In this case, the argument symbol is filename and this symbol is used once in the body of the function, in the call to read.fwf(). This means that the argument to the function is used to select which file the function will read and calculate an average value for.
The value returned by a function is the value of the last expression within the function body. In this case, the return value is the result of the call to the mean() function.
As a simple example, the following code is a call to our calcMonthAvg() function that calculates the overall average temperature from the contents of the file temperature1.txt.
> calcMonthAvg(nasaAirTempFiles[1])
[1] 295.1849
That function call produces exactly the same result as the following code.

           > tempDF <- read.fwf(nasaAirTempFiles[1],
                                 skip=7,
                                 widths=c(-12, rep(7, 24)))
           > tempMatrix <- as.matrix(tempDF)
           > mean(tempMatrix)
The advantage of having defined the function calcMonthAvg() is that it is now possible to calculate the monthly averages from all of the temperature files using sapply().
We supply the vector of filenames, nasaAirTempFiles, and our new function, calcMonthAvg(), and sapply() calls our function for each filename. The USE.NAMES argument is employed here to avoid having large, messy names on each element of the result.
           > sapply(nasaAirTempFiles, calcMonthAvg,
                     USE.NAMES=FALSE)
            [1] 295.1849 295.3175 296.3335 296.9587 297.7286 298.5809
            [7] 299.1863 299.0660 298.4634 298.0564 296.7019 295.9568
           [13] 295.3915 296.1486 296.1087 297.1007 298.3694 298.1970
           [19] 298.4031 298.0682 298.3148 297.3823 296.1304 295.5917
           [25] 295.5562 295.6438 296.8922 297.0823 298.4793 299.3575
           [31] 299.7984 299.7314 299.6090 298.4970 297.9872 296.8453
           [37] 296.9569 296.9354 297.0240 298.0668 299.1821 300.7290
           [43] 300.6998 300.3715 300.1036 299.2269 297.8642 297.2729
           [49] 296.8823 297.4288 297.5762 298.2859 299.1076 299.1938
           [55] 299.0599 299.5424 298.9135 298.2849 297.0981 296.2639
           [61] 296.1943 296.5868 297.5510 298.6106 299.7425 299.5219
           [67] 299.7422 300.3411 299.5781 298.6965 297.0830 296.3813
This is the same as the result that was produced from an explicit loop (see page 382), but it uses only a single call to sapply().
9.11.4 Flashback: Writing functions, writing code, and the DRY principle
The previous example demonstrates that it is useful to be able to define our own functions for use with functions like apply(), lapply(), and sapply(). However, there are many other good reasons for being able to write functions. In particular, functions are useful for organizing code, simplifying code, and for making it easier to maintain code.

For example, the code below reproduces the loop that we wrote in Section 9.11.2, which calculates all 72 monthly averages.
           for (i in 1:72) {
               tempDF <- read.fwf(nasaAirTempFiles[i],
                              skip=7,
                              widths=c(-12,
                                rep(7, 24)))
               tempMatrix <- as.matrix(tempDF)
               avgTemp[i] <- mean(tempMatrix)
           }
We can use the calcMonthAvg() function to make this code much simpler and easier to read.
           for (i in 1:72) {
               avgTemp[i] <- calcMonthAvg(nasaAirTempFiles[i])
}
This sort of use for functions is just an extension of the ideas of laying out and documenting code for the benefit of human readers that we saw in Section 2.4.
A further advantage that we can obtain from writing functions is the ability to reuse our code.
The calcMonthAvg() function nicely encapsulates the code for calculating the overall average temperature from one of these NASA files. If we ever need to perform this calculation in another analysis, rather than writing the code again, we can simply make use of this function again.
In this sense, functions are another example of the DRY principle because a function allows us to create and maintain a single copy of a piece of information—in this case, the computer code to perform a specific task.
9.11.5 Flashback: Debugging
All of the admonitions about writing code in small pieces and changing one thing at a time, which were discussed first in Section 2.6.2, are even more important when writing code in a programming language because the code tends to be more complex.

A specific point that can be made about writing R code, especially code that involves many expressions, is that each individual expression can be run, one at a time, in the R environment. If the final result of a collection of R code is not correct, the first thing to do is to run the individual expressions, one at a time, in order, and check that the result of each expression is correct. This is an excellent way to track down where a problem is occurring.
Matters become far more complicated once we begin to write our own functions, or even if we write loops. If the code within a function is not performing as expected, the debug() function is very useful because it allows us to run the code within a function one expression at a time.
Another useful simple trick, for both functions and loops, is to use the functions from Section 9.10 to write messages to the screen that show the values from intermediate calculations.
Recap
A loop is an expression that repeats a body of code multiple times. Functions allow a piece of code to be reused.
In their simplest form, functions are like macros, simply a recording of a series of steps.
The best way to debug R code is to run it one expression at a time.
9.12 Other software
This chapter has focused on the R language for simple programming tasks.
The tools described in this chapter are the core tools for working with the fundamental data structures in R. In specific areas of research, particularly where data sets have a special format, there may be R packages that provide more sophisticated and convenient tools for working with a specific data format.
A good example of this is the zoo package for working with time series data. Other examples are the packages within the Bioconductor project4 that provide tools for working with the results of microarray experiments.
The choice of R as a data processing tool was based on the fact that R is 

4 http://www.bioconductor.org/

a relatively easy programming language to learn and it has good facilities for manipulating data. R is also an excellent environment for data analysis, so learning to process data with R means that it is possible to prepare and analyze a data set within a single system.
However, there are two major disadvantages to working with data using R: R is slower than many other programming languages, and R holds all data in RAM, so it can be awkward to work with extremely large data sets.
There are a number of R packages that enhance R’s ability to cope gracefully with very large data sets. One approach is to store the data in a relational database and use the packages that allow R to talk to the database software, as described in Section 9.7.8. Several other packages solve the problem by storing data in mass storage rather than RAM and just load data as required into RAM; two examples are the filehash and ff packages.
If R is too slow for a particular task, it may be more appropriate to use a different data processing tool.
There are many alternative programming languages, such as C, Perl, Python, and Java, that will run much faster than R. The trade-off with using one of these programming languages is likely to involve writing more code, and more complex code, in order to produce code that runs faster.
It is also worth mentioning that many simple software tools exist, especially for processing text files. In particular, on Linux systems, there are programs such as sort for sorting files, grep for finding patterns in files, cut for removing pieces from each line of a file, and sed and awk for more powerful rearrangements of file contents. These tools are not as flexible or powerful as R, but they are extremely fast and will work on files of virtually any size. The Cygwin project5 makes these tools available on Windows.
Summary
A programming language provides great flexibility and power because it can be used to control the computer hardware, including accessing data from mass storage, processing data values within RAM, and displaying data on screen.
R is a good programming language for working with data because it is relatively easy to learn and it provides many tools for processing data.
A data structure is the format in which a collection of data values is stored

5 http://www.cygwin.com/

together within RAM. The basic data structures in R are vectors, matrices, data frames, and lists.
Standard data processing tasks include: reading data from external files into RAM; extracting subsets of data; sorting, transforming, aggregating, and reshaping data structures; searching and reformatting text data; and formatting data for display on screen.
